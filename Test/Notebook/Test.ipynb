{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["!pip install diffusers\n","!pip install transformers\n","!pip install accelerate\n","!pip install ftfy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from PIL import Image\n","from torchvision import models, transforms, utils as vutils\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import joblib\n","from torch.utils.data import DataLoader, Dataset, Subset\n","from diffusers import DiffusionPipeline\n","import matplotlib.pyplot as plt\n","import json  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","# Verifica delle GPU disponibili\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","dtype = torch.float16 if device == \"cuda\" else torch.float32\n","\n","# Caricamento della pipeline Stable Diffusion\n","pipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=dtype)\n","pipeline.to(device)\n","\n","# Percorso al file JSON delle annotazioni COCO (modifica il percorso in base alla tua configurazione)\n","coco_annotation_file = '/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_train2017.json'\n","\n","# Funzione per leggere il file JSON delle annotazioni\n","def load_coco_annotations(json_file):\n","    \"\"\"\n","    Carica le annotazioni dal file JSON del dataset COCO.\n","    \n","    Args:\n","    - json_file (str): Il percorso del file JSON con le annotazioni COCO.\n","\n","    Returns:\n","    - dict: Un dizionario con i dati delle annotazioni COCO.\n","    \"\"\"\n","    with open(json_file, 'r') as f:\n","        data = json.load(f)\n","    return data\n","\n","# Carica le annotazioni COCO\n","coco_data = load_coco_annotations(coco_annotation_file)\n","\n","# Mappa degli ID delle categorie alle descrizioni delle categorie\n","categories = {cat['id']: cat['name'] for cat in coco_data['categories']}\n","\n","# Creazione delle directory per salvare le immagini 64x64, 256x256 e originali localmente su Kaggle\n","base_dir = '/kaggle/working/Immagini_stable/'\n","dir_64x64 = os.path.join(base_dir, '64x64')\n","dir_256x256 = os.path.join(base_dir, '256x256')\n","dir_original = os.path.join(base_dir, 'original')\n","\n","# Creazione delle directory se non esistono già\n","os.makedirs(dir_64x64, exist_ok=True)\n","os.makedirs(dir_256x256, exist_ok=True)\n","os.makedirs(dir_original, exist_ok=True)\n","\n","# Estrazione delle annotazioni per le immagini\n","image_annotations = {}\n","for annotation in coco_data['annotations']:\n","    img_id = annotation['image_id']\n","    category_name = categories[annotation['category_id']]\n","    \n","    if img_id in image_annotations:\n","        image_annotations[img_id].append(category_name)\n","    else:\n","        image_annotations[img_id] = [category_name]\n","\n","# Selezione delle seconde 2000 immagini\n","sample_ids = list(image_annotations.keys())[2000:4000]  # Seleziona le immagini dal 2001° al 4000°\n","\n","# Funzione per generare immagini a partire da un prompt\n","def generate_image(prompt):\n","    \"\"\"\n","    Genera un'immagine utilizzando la pipeline Stable Diffusion a partire da un prompt.\n","    \n","    Args:\n","    - prompt (str): Il prompt testuale per generare l'immagine.\n","\n","    Returns:\n","    - image: L'immagine generata come oggetto PIL.Image.\n","    \"\"\"\n","    image = pipeline(prompt, num_inference_steps=8).images[0]\n","    return image.convert(\"RGB\")\n","\n","# Iterazione per generare le seconde 2000 immagini e salvarle\n","for i, img_id in enumerate(sample_ids, start=1):  # Inizia la numerazione da 1\n","    try:\n","        # Crea il prompt dalle categorie associate a questa immagine\n","        prompt = \", \".join(image_annotations[img_id])\n","\n","        # Genera immagine fake usando il prompt\n","        image_fake = generate_image(prompt)\n","\n","        # Ridimensiona l'immagine a 64x64 e 256x256\n","        image_64x64 = image_fake.resize((64, 64))\n","        image_256x256 = image_fake.resize((256, 256))\n","\n","        # Percorsi per salvare le immagini nelle sottocartelle locali\n","        save_path_64 = f'{dir_64x64}/fake_image_{i}.png'\n","        save_path_256 = f'{dir_256x256}/fake_image_{i}.png'\n","        save_path_original = f'{dir_original}/image_{i}.png'  # Nome immagine originale con numerazione da 1\n","\n","        # Salva le immagini nelle rispettive directory\n","        image_64x64.save(save_path_64)\n","        image_256x256.save(save_path_256)\n","        image_fake.save(save_path_original)  # Salva l'immagine originale come image_{i}.png\n","\n","        print(f\"Immagine {i}/2000 generata e salvata in {save_path_64}, {save_path_256}, e {save_path_original}\")\n","\n","    except Exception as e:\n","        print(f\"Errore nella generazione dell'immagine ID {img_id}: {e}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import subprocess\n","from IPython.display import FileLink, display\n","\n","def download_file(path, download_file_name):\n","    os.chdir('/kaggle/working/')\n","    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n","    command = f\"zip {zip_name} {path} -r\"\n","    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n","    if result.returncode != 0:\n","        print(\"Unable to run zip command!\")\n","        print(result.stderr)\n","        return\n","    display(FileLink(f'{download_file_name}.zip'))\n","download_file('/kaggle/working/Immagini_stable', 'out')\n","#download_file('/kaggle/working/Immagini_gan', 'out1')\n","#download_file('/kaggle/working/IS_FIS_results', 'out2')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import shutil\n","import os\n","\n","# Percorso della cartella da eliminare\n","base_dir = '/kaggle/working/metrics_combined_64x64_compression_60.txt'\n","\n","# Controlla se la cartella esiste, quindi eliminala insieme a tutti i suoi contenuti\n","if os.path.exists(base_dir):\n","    shutil.rmtree(base_dir)\n","    print(f\"Tutto il contenuto della cartella {base_dir} è stato eliminato.\")\n","else:\n","    print(f\"La cartella {base_dir} non esiste.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","# Iperparametri per il Generatore\n","image_size = 64  # Dimensione delle immagini generate\n","nz = 100  # Dimensione del vettore latente\n","ngf = 64  # Numero di feature maps nel generatore\n","nc = 3  # Numero di canali nelle immagini di allenamento (RGB)\n","ngpu = torch.cuda.device_count()  # Rileva automaticamente il numero di GPU disponibili\n","\n","# Definizione del modello Generator\n","class Generator(nn.Module):\n","    \"\"\"\n","    Una classe che rappresenta il modello del Generatore per la generazione di immagini.\n","    Il modello utilizza una serie di layer ConvTranspose2d per upscalare il vettore latente \n","    fino a ottenere la dimensione dell'immagine desiderata.\n","\n","    Args:\n","    - ngpu (int): Numero di GPU disponibili per il calcolo parallelo.\n","    \"\"\"\n","    def __init__(self, ngpu):\n","        super(Generator, self).__init__()\n","        self.ngpu = ngpu\n","        self.main = nn.Sequential(\n","            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),  # Layer 1\n","            nn.BatchNorm2d(ngf * 8),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),  # Layer 2\n","            nn.BatchNorm2d(ngf * 4),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),  # Layer 3\n","            nn.BatchNorm2d(ngf * 2),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),  # Layer 4\n","            nn.BatchNorm2d(ngf),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),  # Layer finale\n","            nn.Tanh()  # Output scalato tra -1 e 1\n","        )\n","\n","    def forward(self, input):\n","        \"\"\"\n","        Passaggio forward del Generatore. Riceve un vettore latente e produce un'immagine.\n","        \n","        Args:\n","        - input (Tensor): Un batch di vettori latenti con forma [batch_size, nz, 1, 1].\n","\n","        Returns:\n","        - Tensor: Immagini generate.\n","        \"\"\"\n","        return self.main(input)\n","\n","# Inizializzazione del generatore\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","netG = Generator(ngpu=ngpu)  # Creazione di un'istanza del modello Generator\n","\n","# Usa DataParallel se sono disponibili più GPU\n","if ngpu > 1:\n","    netG = nn.DataParallel(netG, list(range(ngpu)))\n","\n","netG = netG.to(device)  # Sposta il modello su GPU (o CPU se non disponibile)\n","\n","# Carica i pesi del generatore da un checkpoint\n","checkpoint_path_generator = '/kaggle/input/checkpoint-generatore/checkpoint_generator.pth'\n","state_dict = torch.load(checkpoint_path_generator, map_location=device)\n","\n","# Controlla se lo state_dict proviene da un modello DataParallel e modifica se necessario\n","new_state_dict = {}\n","for key, value in state_dict.items():\n","    if ngpu > 1 and not key.startswith(\"module.\"):\n","        new_state_dict[\"module.\" + key] = value  # Aggiungi 'module.' come prefisso se si usa DataParallel\n","    elif ngpu == 1 and key.startswith(\"module.\"):\n","        new_state_dict[key[len(\"module.\"):]] = value  # Rimuovi 'module.' se non si usa DataParallel\n","    else:\n","        new_state_dict[key] = value\n","\n","# Carica il nuovo state dict modificato nel modello\n","netG.load_state_dict(new_state_dict)\n","\n","# Imposta il generatore in modalità di valutazione\n","netG.eval()\n","\n","# Genera vettori latenti (rumore casuale)\n","fixed_noise = torch.randn(2000, nz, 1, 1, device=device)  # Genera 2000 vettori di rumore casuale\n","\n","# Genera 2000 immagini fake dal generatore\n","with torch.no_grad(): \n","    fake_images = netG(fixed_noise).detach().cpu()  # Genera immagini e spostale su CPU\n","\n","# Imposta la directory per salvare le immagini generate\n","save_directory = '/kaggle/working/Immagini_gan/'\n","dir_64x64 = os.path.join(save_directory, '64x64')\n","dir_256x256 = os.path.join(save_directory, '256x256')\n","\n","# Crea le directory se non esistono\n","os.makedirs(dir_64x64, exist_ok=True)\n","os.makedirs(dir_256x256, exist_ok=True)\n","\n","# Salva ogni immagine generata in versioni 64x64 e 256x256\n","for i in range(fake_images.size(0)):\n","    image_64_path = os.path.join(dir_64x64, f'generated_image_{i+1}.png')\n","    image_256_path = os.path.join(dir_256x256, f'generated_image_{i+1}.png')\n","\n","    # Salva la versione 64x64\n","    vutils.save_image(fake_images[i], image_64_path, normalize=True)\n","\n","    # Ridimensiona l'immagine a 256x256 e salvala\n","    image_256 = F.interpolate(fake_images[i].unsqueeze(0), size=(256, 256)).squeeze(0)\n","    vutils.save_image(image_256, image_256_path, normalize=True)\n","\n","    print(f\"Immagini salvate: {image_64_path}, {image_256_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","# Percorsi per le immagini GAN e Stable Diffusion (tutte considerate \"generate\")\n","gan_images_dir_64 = '/kaggle/working/Immagini_gan/64x64'\n","gan_images_dir_256 = '/kaggle/working/Immagini_gan/256x256'\n","stable_images_dir_64 = '/kaggle/working/Immagini_stable/64x64'\n","stable_images_dir_256 = '/kaggle/working/Immagini_stable/256x256'\n","\n","# Percorsi per salvare i risultati IS e FIS\n","results_dir = '/kaggle/working/IS_FIS_results'\n","os.makedirs(results_dir, exist_ok=True)\n","\n","# Dispositivo GPU/CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Percorso del dataset COCO (sostituisci con il percorso corretto)\n","coco_annotation_file = '/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_train2017.json'\n","coco_images_dir = '/kaggle/input/coco-2017-dataset/coco2017/train2017'  # Directory delle immagini COCO\n","\n","# Funzione per caricare immagini dal dataset COCO\n","def load_coco_images(coco_annotation_file, coco_images_dir, start_idx, end_idx, image_size):\n","    \"\"\"\n","    Carica un insieme di immagini dal dataset COCO tra gli indici specificati, ridimensionandole a una data dimensione.\n","    \n","    Args:\n","    - coco_annotation_file (str): Percorso al file JSON delle annotazioni COCO.\n","    - coco_images_dir (str): Directory delle immagini COCO.\n","    - start_idx (int): Indice iniziale delle immagini da caricare.\n","    - end_idx (int): Indice finale delle immagini da caricare.\n","    - image_size (int): Dimensione a cui ridimensionare le immagini.\n","\n","    Returns:\n","    - list: Lista di immagini caricate e ridimensionate.\n","    \"\"\"\n","    with open(coco_annotation_file, 'r') as f:\n","        coco_data = json.load(f)\n","    \n","    image_ids = [img['file_name'] for img in coco_data['images'][start_idx:end_idx]][:1000]\n","    images = []\n","    \n","    for img_id in image_ids:\n","        img_path = os.path.join(coco_images_dir, img_id)\n","        img = Image.open(img_path).convert(\"RGB\")\n","        img = img.resize((image_size, image_size))\n","        images.append(img)\n","    \n","    return images\n","\n","# Funzione per caricare immagini generate da una directory (limita a 1000 immagini)\n","def load_images_from_dir(directory, image_size, limit=1000):\n","    \"\"\"\n","    Carica le immagini generate da una directory e le ridimensiona a una data dimensione.\n","    \n","    Args:\n","    - directory (str): Directory da cui caricare le immagini generate.\n","    - image_size (int): Dimensione a cui ridimensionare le immagini.\n","    - limit (int): Numero massimo di immagini da caricare.\n","\n","    Returns:\n","    - list: Lista di immagini caricate e ridimensionate.\n","    \"\"\"\n","    images = []\n","    for filename in os.listdir(directory)[:limit]:\n","        if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n","            img_path = os.path.join(directory, filename)\n","            img = Image.open(img_path).convert(\"RGB\")\n","            img = img.resize((image_size, image_size))\n","            images.append(img)\n","    return images\n","\n","# Preprocessing per InceptionV3 (resize to 299x299) e ResNet (resize alle dimensioni originali)\n","def preprocess_images(images, image_size, for_inception=False):\n","    \"\"\"\n","    Preprocessa un insieme di immagini per essere utilizzate in un modello di deep learning.\n","    \n","    Args:\n","    - images (list): Lista di immagini da preprocessare.\n","    - image_size (int): Dimensione a cui ridimensionare le immagini.\n","    - for_inception (bool): Se True, ridimensiona le immagini a 299x299 per InceptionV3.\n","\n","    Returns:\n","    - Tensor: Batch di immagini preprocessate.\n","    \"\"\"\n","    if for_inception:\n","        preprocess = transforms.Compose([\n","            transforms.Resize(299),\n","            transforms.CenterCrop(299),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","        ])\n","    else:\n","        preprocess = transforms.Compose([\n","            transforms.Resize((image_size, image_size)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","        ])\n","    \n","    return torch.stack([preprocess(img) for img in images]).to(device)\n","\n","# Funzione per calcolare l'Inception Score (IS)\n","def calculate_inception_score(images, splits=10):\n","    \"\"\"\n","    Calcola l'Inception Score per un insieme di immagini.\n","    \n","    Args:\n","    - images (Tensor): Batch di immagini preprocessate.\n","    - splits (int): Numero di suddivisioni per calcolare IS.\n","\n","    Returns:\n","    - tuple: Media e deviazione standard dell'Inception Score.\n","    \"\"\"\n","    inception_model = models.inception_v3(pretrained=True, transform_input=False).to(device)\n","    inception_model.eval()\n","\n","    def get_pred(x):\n","        x = inception_model(x)\n","        return torch.nn.functional.softmax(x, dim=1).data.cpu().numpy()\n","\n","    N = len(images)\n","    batch_size = 32\n","    preds = np.zeros((N, 1000))\n","\n","    for i in range(0, N, batch_size):\n","        batch_images = images[i:i + batch_size]\n","        preds[i:i + batch_size] = get_pred(batch_images)\n","\n","    scores = []\n","    for i in range(splits):\n","        part = preds[i * (N // splits): (i + 1) * (N // splits), :]\n","        py = np.mean(part, axis=0)\n","        kl_div = part * (np.log(part) - np.log(py))\n","        kl_div = np.sum(kl_div, axis=1)\n","        scores.append(np.exp(np.mean(kl_div)))\n","\n","    return np.mean(scores), np.std(scores)\n","\n","# Funzione per estrarre le feature dalle immagini usando ResNet18\n","def extract_features(images, model):\n","    \"\"\"\n","    Estrae le feature da un batch di immagini utilizzando un modello pre-addestrato.\n","    \n","    Args:\n","    - images (Tensor): Batch di immagini preprocessate.\n","    - model (torch.nn.Module): Modello pre-addestrato per l'estrazione delle feature.\n","\n","    Returns:\n","    - Tensor: Feature estratte dalle immagini.\n","    \"\"\"\n","    with torch.no_grad():\n","        features = model(images)\n","    return features\n","\n","# Funzione per calcolare il Feature Importance Score (FIS)\n","def calculate_fis(real_images, generated_images, model, batch_size=32):\n","    \"\"\"\n","    Calcola il Feature Importance Score (FIS) tra immagini reali e generate.\n","    \n","    Args:\n","    - real_images (Tensor): Batch di immagini reali.\n","    - generated_images (Tensor): Batch di immagini generate.\n","    - model (torch.nn.Module): Modello pre-addestrato per l'estrazione delle feature.\n","    - batch_size (int): Dimensione della batch per l'elaborazione.\n","\n","    Returns:\n","    - float: Punteggio medio del FIS.\n","    \"\"\"\n","    total_distance = 0.0\n","    num_batches = 0\n","\n","    for i in range(0, len(real_images), batch_size):\n","        real_batch = real_images[i:i + batch_size]\n","        generated_batch = generated_images[i:i + batch_size]\n","        \n","        real_features = extract_features(real_batch, model)\n","        generated_features = extract_features(generated_batch, model)\n","        \n","        distance = torch.norm(real_features - generated_features, dim=1).mean().item()\n","        total_distance += distance\n","        num_batches += 1\n","\n","    return total_distance / num_batches\n","\n","# Funzione per salvare il grafico FIS e i risultati finali\n","def save_fis_results(fis_score, fis_txt_file, fis_plot_file):\n","    \"\"\"\n","    Salva i risultati del Feature Importance Score (FIS) in un file di testo e un grafico.\n","    \n","    Args:\n","    - fis_score (float): Valore del punteggio FIS.\n","    - fis_txt_file (str): Percorso del file di testo per salvare il punteggio.\n","    - fis_plot_file (str): Percorso del file per salvare il grafico FIS.\n","    \"\"\"\n","    with open(fis_txt_file, 'w') as f:\n","        f.write(f\"Feature Importance Score (FIS): {fis_score:.4f}\\n\")\n","\n","    plt.figure(figsize=(6, 4))\n","    plt.plot([1], [fis_score], marker='o', linestyle='-', color='b')\n","    plt.title(\"Feature Importance Score (FIS) Finale\")\n","    plt.xlabel(\"Valore Finale\")\n","    plt.ylabel(\"Feature Importance Score (FIS)\")\n","    plt.grid(True)\n","\n","    plt.savefig(fis_plot_file)\n","    plt.close()\n","\n","# Funzione principale per calcolare IS e FIS\n","def calculate_is_and_fis(image_size, gan_dir, stable_dir, results_prefix, batch_size=32):\n","    \"\"\"\n","    Calcola l'Inception Score (IS) e il Feature Importance Score (FIS) per immagini reali e generate.\n","    \n","    Args:\n","    - image_size (int): Dimensione delle immagini da utilizzare.\n","    - gan_dir (str): Directory delle immagini GAN generate.\n","    - stable_dir (str): Directory delle immagini Stable Diffusion generate.\n","    - results_prefix (str): Prefisso per i file di risultati.\n","    - batch_size (int): Dimensione della batch per l'elaborazione.\n","\n","    Returns:\n","    - None\n","    \"\"\"\n","    # Caricamento delle immagini reali dal dataset COCO\n","    real_images = load_coco_images(coco_annotation_file, coco_images_dir, 2000, 4000, image_size)\n","    real_images_tensor = preprocess_images(real_images[:1000], image_size)  # Usa solo 1000 immagini reali\n","\n","    # Caricamento delle immagini generate (GAN e Stable)\n","    gan_images = load_images_from_dir(gan_dir, image_size, limit=1000)\n","    stable_images = load_images_from_dir(stable_dir, image_size, limit=1000)\n","    \n","    # Unisci tutte le immagini generate (GAN + Stable)\n","    generated_images = gan_images + stable_images\n","    generated_images_tensor = preprocess_images(generated_images[:1000], image_size)  # Usa solo 1000 immagini generate\n","\n","    # Inception Score per immagini generate\n","    generated_images_tensor_299 = preprocess_images(generated_images[:1000], image_size, for_inception=True)  # Resize a 299x299\n","    mean_is_gan, std_is_gan = calculate_inception_score(generated_images_tensor_299)\n","    print(f\"Inception Score Generated ({image_size}x{image_size}): Mean = {mean_is_gan}, Std = {std_is_gan}\")\n","\n","    # Salva i risultati IS in un file di testo\n","    is_results_file = os.path.join(results_dir, f'{results_prefix}_inception_score_results.txt')\n","    with open(is_results_file, 'w') as f:\n","        f.write(f\"Inception Score ({image_size}x{image_size}): Mean = {mean_is_gan}, Std = {std_is_gan}\\n\")\n","\n","    # Caricamento del modello ResNet18 per calcolare FIS\n","    resnet_model = models.resnet18(pretrained=True).eval().to(device)\n","    resnet_model = nn.Sequential(*list(resnet_model.children())[:-1])  # Rimuovi il layer di classificazione\n","\n","    # Feature Importance Score (FIS) per immagini generate (considerando tutte le batch)\n","    fis_gan = calculate_fis(real_images_tensor, generated_images_tensor, resnet_model, batch_size=batch_size)\n","    print(f\"Feature Importance Score Generated ({image_size}x{image_size}): {fis_gan}\")\n","\n","    # Salva i risultati FIS\n","    fis_results_file = os.path.join(results_dir, f'{results_prefix}_fis_results.txt')\n","    fis_plot_file = os.path.join(results_dir, f'{results_prefix}_fis_plot.png')\n","    save_fis_results(fis_gan, fis_results_file, fis_plot_file)\n","\n","# Calcolo per 64x64 (GAN e Stable Diffusion come generate, COCO come reali)\n","calculate_is_and_fis(64, gan_images_dir_64, stable_images_dir_64, \"64x64\", batch_size=16)\n","\n","# Calcolo per 256x256 (GAN e Stable Diffusion come generate, COCO come reali)\n","calculate_is_and_fis(256, gan_images_dir_256, stable_images_dir_256, \"256x256\", batch_size=8)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["\n","\n","# Configurazione per l'uso di GPU o CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","ngpu = torch.cuda.device_count()\n","\n","# Carica il modello VGG16 pre-addestrato\n","vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n","vgg16.classifier = vgg16.classifier[:-1]  # Rimuovi l'ultimo strato (strato di classificazione finale)\n","\n","# Se sono disponibili più GPU, usa DataParallel\n","if ngpu > 1:\n","    vgg16 = torch.nn.DataParallel(vgg16)\n","\n","vgg16.eval().to(device)  # Imposta il modello in modalità di valutazione e spostalo su GPU/CPU\n","\n","# Funzione per preprocessare le immagini (64x64 o 256x256)\n","def preprocess_image(image, size=64):\n","    \"\"\"\n","    Preprocessa un'immagine ridimensionandola e normalizzandola.\n","    \n","    Args:\n","    - image (PIL.Image): Immagine da preprocessare.\n","    - size (int): Dimensione a cui ridimensionare l'immagine (default 64).\n","    \n","    Returns:\n","    - Tensor: Immagine preprocessata come Tensor.\n","    \"\"\"\n","    transform = transforms.Compose([\n","        transforms.Resize((size, size)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","    return transform(image)\n","\n","# Dataset personalizzato per caricare le immagini da una cartella\n","class ImageDataset(Dataset):\n","    \"\"\"\n","    Classe per creare un dataset personalizzato che carica immagini da una directory e le preprocessa.\n","    \n","    Args:\n","    - image_dir (str): Percorso alla directory delle immagini.\n","    - size (int): Dimensione a cui ridimensionare le immagini.\n","    \"\"\"\n","    def __init__(self, image_dir, size):\n","        self.image_dir = image_dir\n","        self.size = size\n","        self.image_names = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.image_dir, self.image_names[idx])\n","        image = Image.open(img_path).convert('RGB')\n","        image = preprocess_image(image, self.size)\n","        return image, self.image_names[idx]\n","\n","# Funzione per estrarre le caratteristiche con VGG16\n","def extract_features(images, model):\n","    \"\"\"\n","    Estrae le caratteristiche delle immagini utilizzando il modello VGG16.\n","    \n","    Args:\n","    - images (Tensor): Batch di immagini preprocessate.\n","    - model (torch.nn.Module): Modello pre-addestrato (VGG16) per l'estrazione delle caratteristiche.\n","    \n","    Returns:\n","    - np.ndarray: Caratteristiche estratte dalle immagini.\n","    \"\"\"\n","    images = images.to(device)\n","    with torch.no_grad():  # Disabilita il calcolo dei gradienti per risparmiare memoria\n","        features = model(images)\n","    return features.flatten(1).cpu().numpy()\n","\n","# Funzione per calcolare e salvare le metriche\n","def calculate_metrics(y_true, y_pred, save_path):\n","    \"\"\"\n","    Calcola l'accuratezza, precisione, richiamo e F1 score e salva i risultati in un file.\n","    \n","    Args:\n","    - y_true (list): Lista dei valori veri (reali).\n","    - y_pred (list): Lista delle predizioni (generate dal modello).\n","    - save_path (str): Percorso del file dove salvare i risultati.\n","    \"\"\"\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","\n","    # Salva i risultati in un file di testo\n","    with open(save_path, 'w') as f:\n","        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n","        f.write(f\"Precision: {precision:.4f}\\n\")\n","        f.write(f\"Recall: {recall:.4f}\\n\")\n","        f.write(f\"F1 Score: {f1:.4f}\\n\")\n","\n","    print(f\"Metrics saved to {save_path}\")\n","\n","# Funzione per processare le immagini in batch\n","def process_images_in_batches(dataloader_real, dataloader_generated, model, svm, result_file):\n","    \"\"\"\n","    Processa le immagini reali e generate in batch, estrae le caratteristiche e confronta le predizioni\n","    utilizzando un classificatore SVM.\n","\n","    Args:\n","    - dataloader_real (DataLoader): Dataloader per le immagini reali.\n","    - dataloader_generated (DataLoader): Dataloader per le immagini generate.\n","    - model (torch.nn.Module): Modello pre-addestrato per l'estrazione delle caratteristiche.\n","    - svm (sklearn.svm.SVC): Classificatore SVM per confrontare le caratteristiche.\n","    - result_file (str): Percorso del file dove salvare i risultati delle metriche.\n","    \"\"\"\n","    y_true = []\n","    y_pred = []\n","\n","    for real_batch, _ in dataloader_real:\n","        real_batch = real_batch.to(device)\n","        real_features = extract_features(real_batch, model)\n","\n","        for gen_batch, _ in dataloader_generated:\n","            gen_batch = gen_batch.to(device)\n","            gen_features = extract_features(gen_batch, model)\n","\n","            # Combina le caratteristiche reali e generate\n","            combined_features = torch.cat([torch.tensor(real_features), torch.tensor(gen_features)], dim=0).numpy()\n","\n","            # Predizioni con il classificatore SVM\n","            predictions = svm.predict(combined_features)\n","\n","            # Aggiungi i valori veri (1 per reale, 0 per generato) e le predizioni\n","            y_true.extend([1] * len(real_batch) + [0] * len(gen_batch))\n","            y_pred.extend(predictions)\n","\n","            # Libera la memoria GPU dopo ogni batch\n","            torch.cuda.empty_cache()\n","\n","    calculate_metrics(y_true, y_pred, result_file)\n","\n","# Percorsi per i checkpoint SVM\n","svm_checkpoint_64 = '/kaggle/input/checkpoint-classificatore-6464/svm_classifier_64.pkl'\n","\n","# Percorsi per le immagini ridimensionate\n","coco_dir = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n","stable_64x64_dir = '/kaggle/input/immagini-gan/kaggle/working/Immagini_gan/64x64'\n","gan_64x64_dir = '/kaggle/input/immagini-gan/kaggle/working/Immagini_gan/64x64'\n","\n","# Batch size ridotto per ridurre l'uso della memoria GPU\n","batch_size = 128  # Ridotto per evitare errori di memoria\n","\n","# Dataset e DataLoader per le immagini COCO 64x64 (limite a 1000 immagini)\n","dataset_coco_64 = ImageDataset(coco_dir, 64)\n","dataset_coco_64 = Subset(dataset_coco_64, range(2000))\n","dataloader_coco_64 = DataLoader(dataset_coco_64, batch_size=batch_size, shuffle=False)\n","\n","# Dataset e DataLoader per le immagini Stable 64x64 (limite a 1000 immagini)\n","dataset_stable_64 = ImageDataset(stable_64x64_dir, 64)\n","dataset_stable_64 = Subset(dataset_stable_64, range(2000))\n","dataloader_stable_64 = DataLoader(dataset_stable_64, batch_size=batch_size, shuffle=False)\n","\n","# Dataset e DataLoader per le immagini GAN 64x64 (limite a 1000 immagini)\n","dataset_gan_64 = ImageDataset(gan_64x64_dir, 64)\n","dataset_gan_64 = Subset(dataset_gan_64, range(2000))\n","dataloader_gan_64 = DataLoader(dataset_gan_64, batch_size=batch_size, shuffle=False)\n","\n","# Carica il classificatore SVM per 64x64\n","svm_64 = joblib.load(svm_checkpoint_64)\n","\n","# Confronto tra COCO e Stable Diffusion (64x64)\n","print(\"Processing COCO vs Stable Diffusion (64x64)...\")\n","process_images_in_batches(dataloader_coco_64, dataloader_stable_64, vgg16, svm_64, '/kaggle/working/metrics_coco_stable_6464.txt')\n","\n","# Confronto tra COCO e GAN (64x64)\n","print(\"Processing COCO vs GAN (64x64)...\")\n","process_images_in_batches(dataloader_coco_64, dataloader_gan_64, vgg16, svm_64, '/kaggle/working/metrics_coco_gan_6464.txt')\n","\n","print(\"Processing complete.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","# Configurazione per l'uso di GPU o CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","ngpu = torch.cuda.device_count()\n","\n","# Carica il modello VGG16 pre-addestrato\n","vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n","vgg16.classifier = vgg16.classifier[:-1]  # Rimuovi l'ultimo strato di classificazione\n","\n","# Se ci sono più GPU, utilizza DataParallel per parallelizzare il carico di lavoro\n","if ngpu > 1:\n","    vgg16 = torch.nn.DataParallel(vgg16)\n","vgg16.eval().to(device)  # Imposta il modello in modalità di valutazione e spostalo su GPU/CPU\n","\n","# Funzione per preprocessare le immagini (256x256)\n","def preprocess_image(image, size=256):\n","    \"\"\"\n","    Preprocessa un'immagine ridimensionandola a una dimensione specificata e normalizzandola.\n","    \n","    Args:\n","    - image (PIL.Image): Immagine da preprocessare.\n","    - size (int): Dimensione a cui ridimensionare l'immagine (default: 256).\n","    \n","    Returns:\n","    - Tensor: Immagine preprocessata come Tensor.\n","    \"\"\"\n","    transform = transforms.Compose([\n","        transforms.Resize((size, size)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","    return transform(image)\n","\n","# Dataset personalizzato per caricare le immagini da una cartella\n","class ImageDataset(Dataset):\n","    \"\"\"\n","    Classe per creare un dataset personalizzato che carica immagini da una directory e le preprocessa.\n","    \n","    Args:\n","    - image_dir (str): Percorso alla directory delle immagini.\n","    - size (int): Dimensione a cui ridimensionare le immagini.\n","    \"\"\"\n","    def __init__(self, image_dir, size):\n","        self.image_dir = image_dir\n","        self.size = size\n","        self.image_names = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.image_dir, self.image_names[idx])\n","        image = Image.open(img_path).convert('RGB')\n","        image = preprocess_image(image, self.size)\n","        return image, self.image_names[idx]\n","\n","# Funzione per estrarre le caratteristiche con VGG16\n","def extract_features(images, model):\n","    \"\"\"\n","    Estrae le caratteristiche dalle immagini utilizzando il modello VGG16.\n","    \n","    Args:\n","    - images (Tensor): Batch di immagini preprocessate.\n","    - model (torch.nn.Module): Modello pre-addestrato per l'estrazione delle caratteristiche (VGG16).\n","    \n","    Returns:\n","    - np.ndarray: Caratteristiche estratte dalle immagini.\n","    \"\"\"\n","    images = images.to(device)\n","    with torch.no_grad():  # Disabilita il calcolo dei gradienti per risparmiare memoria\n","        features = model(images)\n","    return features.flatten(1).cpu().numpy()\n","\n","# Funzione per calcolare e salvare le metriche\n","def calculate_metrics(y_true, y_pred, save_path):\n","    \"\"\"\n","    Calcola l'accuratezza, precisione, richiamo e F1 score e salva i risultati in un file di testo.\n","    \n","    Args:\n","    - y_true (list): Lista dei valori reali (1 per reale, 0 per generato).\n","    - y_pred (list): Lista delle predizioni effettuate dal modello.\n","    - save_path (str): Percorso del file dove salvare i risultati.\n","    \"\"\"\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","\n","    # Salva i risultati in un file di testo\n","    with open(save_path, 'w') as f:\n","        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n","        f.write(f\"Precision: {precision:.4f}\\n\")\n","        f.write(f\"Recall: {recall:.4f}\\n\")\n","        f.write(f\"F1 Score: {f1:.4f}\\n\")\n","\n","    print(f\"Metrics saved to {save_path}\")\n","\n","# Funzione per processare le immagini in batch\n","def process_images_in_batches(dataloader_real, dataloader_generated, model, svm, result_file):\n","    \"\"\"\n","    Processa le immagini reali e generate in batch, estrae le caratteristiche e confronta le predizioni\n","    utilizzando un classificatore SVM.\n","\n","    Args:\n","    - dataloader_real (DataLoader): Dataloader per le immagini reali.\n","    - dataloader_generated (DataLoader): Dataloader per le immagini generate.\n","    - model (torch.nn.Module): Modello pre-addestrato per l'estrazione delle caratteristiche.\n","    - svm (sklearn.svm.SVC): Classificatore SVM per confrontare le caratteristiche.\n","    - result_file (str): Percorso del file dove salvare i risultati delle metriche.\n","    \"\"\"\n","    y_true = []\n","    y_pred = []\n","\n","    for real_batch, _ in dataloader_real:\n","        real_batch = real_batch.to(device)\n","        real_features = extract_features(real_batch, model)\n","\n","        for gen_batch, _ in dataloader_generated:\n","            gen_batch = gen_batch.to(device)\n","            gen_features = extract_features(gen_batch, model)\n","\n","            # Combina le caratteristiche reali e generate\n","            combined_features = torch.cat([torch.tensor(real_features), torch.tensor(gen_features)], dim=0).numpy()\n","\n","            # Predizioni con il classificatore SVM\n","            predictions = svm.predict(combined_features)\n","\n","            # Aggiungi i valori veri (1 per reale, 0 per generato) e le predizioni\n","            y_true.extend([1] * len(real_batch) + [0] * len(gen_batch))\n","            y_pred.extend(predictions)\n","\n","            # Libera la memoria GPU dopo ogni batch\n","            torch.cuda.empty_cache()\n","\n","    calculate_metrics(y_true, y_pred, result_file)\n","\n","# Percorsi per i checkpoint SVM\n","svm_checkpoint_256 = '/kaggle/input/checkpoint-classificatore-256256/svm_classifier_256.pkl'\n","\n","# Percorsi per le immagini ridimensionate\n","coco_dir = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n","stable_256x256_dir = '/kaggle/input/immagini-stable/kaggle/working/Immagini_stable/256x256'\n","gan_256x256_dir = '/kaggle/input/immagini-gan/kaggle/working/Immagini_gan/256x256'\n","\n","# Batch size per l'uso ottimale della memoria GPU\n","batch_size = 128\n","\n","# Dataset e DataLoader per le immagini COCO 256x256 (limite a 2000 immagini)\n","dataset_coco_256 = ImageDataset(coco_dir, 256)\n","dataset_coco_256 = Subset(dataset_coco_256, range(2000))\n","dataloader_coco_256 = DataLoader(dataset_coco_256, batch_size=batch_size, shuffle=False)\n","\n","# Dataset e DataLoader per le immagini Stable 256x256 (limite a 2000 immagini)\n","dataset_stable_256 = ImageDataset(stable_256x256_dir, 256)\n","dataset_stable_256 = Subset(dataset_stable_256, range(2000))\n","dataloader_stable_256 = DataLoader(dataset_stable_256, batch_size=batch_size, shuffle=False)\n","\n","# Dataset e DataLoader per le immagini GAN 256x256 (limite a 2000 immagini)\n","dataset_gan_256 = ImageDataset(gan_256x256_dir, 256)\n","dataset_gan_256 = Subset(dataset_gan_256, range(2000))\n","dataloader_gan_256 = DataLoader(dataset_gan_256, batch_size=batch_size, shuffle=False)\n","\n","# Carica il classificatore SVM per 256x256\n","svm_256 = joblib.load(svm_checkpoint_256)\n","\n","# Confronto tra COCO e Stable Diffusion (256x256)\n","print(\"Processing COCO vs Stable Diffusion (256x256)...\")\n","process_images_in_batches(dataloader_coco_256, dataloader_stable_256, vgg16, svm_256, '/kaggle/working/metrics_coco_stable_256256.txt')\n","\n","# Confronto tra COCO e GAN (256x256)\n","print(\"Processing COCO vs GAN (256x256)...\")\n","process_images_in_batches(dataloader_coco_256, dataloader_gan_256, vgg16, svm_256, '/kaggle/working/metrics_coco_gan_256256.txt')\n","\n","print(\"Processing complete.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","# Configurazione per l'uso di GPU o CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","ngpu = torch.cuda.device_count()\n","\n","# Carica il modello VGG16 pre-addestrato\n","vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n","vgg16.classifier = vgg16.classifier[:-1]  # Rimuovi l'ultimo strato di classificazione\n","\n","# Se ci sono più GPU, utilizza DataParallel per parallelizzare il carico di lavoro\n","if ngpu > 1:\n","    vgg16 = torch.nn.DataParallel(vgg16)\n","vgg16.eval().to(device)  # Imposta il modello in modalità di valutazione e spostalo su GPU/CPU\n","\n","# Carica il classificatore SVM\n","svm_checkpoint = '/kaggle/input/checkpoint-classificatore-6464/svm_classifier_64.pkl'\n","svm_classifier = joblib.load(svm_checkpoint)\n","\n","# Funzione per preprocessare le immagini (64x64)\n","def preprocess_image(image, size=64):\n","    \"\"\"\n","    Preprocessa un'immagine ridimensionandola a una dimensione specificata e normalizzandola.\n","    \n","    Args:\n","    - image (PIL.Image): Immagine da preprocessare.\n","    - size (int): Dimensione a cui ridimensionare l'immagine (default 64).\n","    \n","    Returns:\n","    - Tensor: Immagine preprocessata come Tensor.\n","    \"\"\"\n","    transform = transforms.Compose([\n","        transforms.Resize((size, size)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","    return transform(image)\n","\n","# Dataset personalizzato per caricare le immagini da una cartella\n","class ImageDataset(Dataset):\n","    \"\"\"\n","    Classe per creare un dataset personalizzato che carica immagini da una directory e le preprocessa.\n","    \n","    Args:\n","    - image_dir (str): Percorso alla directory delle immagini.\n","    - size (int): Dimensione a cui ridimensionare le immagini.\n","    \"\"\"\n","    def __init__(self, image_dir, size):\n","        self.image_dir = image_dir\n","        self.size = size\n","        self.image_names = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.image_dir, self.image_names[idx])\n","        image = Image.open(img_path).convert('RGB')\n","        image = preprocess_image(image, self.size)\n","        return image, self.image_names[idx]\n","\n","# Funzione per estrarre le caratteristiche con VGG16\n","def extract_features(images, model):\n","    \"\"\"\n","    Estrae le caratteristiche dalle immagini utilizzando il modello VGG16.\n","    \n","    Args:\n","    - images (Tensor): Batch di immagini preprocessate.\n","    - model (torch.nn.Module): Modello pre-addestrato per l'estrazione delle caratteristiche (VGG16).\n","    \n","    Returns:\n","    - np.ndarray: Caratteristiche estratte dalle immagini.\n","    \"\"\"\n","    images = images.to(device)\n","    with torch.no_grad():  # Disabilita il calcolo dei gradienti per risparmiare memoria\n","        features = model(images)\n","    return features.flatten(1).cpu().numpy()\n","\n","# Funzione per calcolare e salvare le metriche\n","def calculate_metrics(y_true, y_pred, save_path):\n","    \"\"\"\n","    Calcola l'accuratezza, precisione, richiamo e F1 score e salva i risultati in un file di testo.\n","    \n","    Args:\n","    - y_true (list): Lista dei valori reali (1 per reale, 0 per generato).\n","    - y_pred (list): Lista delle predizioni effettuate dal modello.\n","    - save_path (str): Percorso del file dove salvare i risultati.\n","    \"\"\"\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","\n","    # Salva i risultati in un file di testo\n","    with open(save_path, 'w') as f:\n","        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n","        f.write(f\"Precision: {precision:.4f}\\n\")\n","        f.write(f\"Recall: {recall:.4f}\\n\")\n","        f.write(f\"F1 Score: {f1:.4f}\\n\")\n","\n","    print(f\"Metrics saved to {save_path}\")\n","\n","# Funzione per processare le immagini e calcolare le metriche\n","def process_images(real_loader, stable_loader, gan_loader, model, classifier, result_file):\n","    \"\"\"\n","    Processa le immagini reali, Stable Diffusion e GAN, estrae le caratteristiche e confronta le predizioni\n","    utilizzando un classificatore SVM.\n","\n","    Args:\n","    - real_loader (DataLoader): Dataloader per le immagini reali.\n","    - stable_loader (DataLoader): Dataloader per le immagini Stable Diffusion.\n","    - gan_loader (DataLoader): Dataloader per le immagini GAN.\n","    - model (torch.nn.Module): Modello pre-addestrato per l'estrazione delle caratteristiche.\n","    - classifier (sklearn.svm.SVC): Classificatore SVM per confrontare le caratteristiche.\n","    - result_file (str): Percorso del file dove salvare i risultati delle metriche.\n","    \"\"\"\n","    y_true = []\n","    y_pred = []\n","\n","    # Processa immagini reali\n","    for real_batch, _ in real_loader:\n","        real_batch = real_batch.to(device)\n","        real_features = extract_features(real_batch, model)\n","        predictions_real = classifier.predict(real_features)\n","        y_true.extend([1] * len(real_batch))  # Label 1 per immagini reali\n","        y_pred.extend(predictions_real)\n","\n","    # Processa immagini generate da Stable Diffusion\n","    for stable_batch, _ in stable_loader:\n","        stable_batch = stable_batch.to(device)\n","        stable_features = extract_features(stable_batch, model)\n","        predictions_stable = classifier.predict(stable_features)\n","        y_true.extend([0] * len(stable_batch))  # Label 0 per immagini generate\n","        y_pred.extend(predictions_stable)\n","\n","    # Processa immagini generate da GAN\n","    for gan_batch, _ in gan_loader:\n","        gan_batch = gan_batch.to(device)\n","        gan_features = extract_features(gan_batch, model)\n","        predictions_gan = classifier.predict(gan_features)\n","        y_true.extend([0] * len(gan_batch))  # Label 0 per immagini generate\n","        y_pred.extend(predictions_gan)\n","\n","    # Calcola le metriche\n","    calculate_metrics(y_true, y_pred, result_file)\n","\n","# Percorsi per le immagini\n","coco_dir = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n","stable_64x64_dir = '/kaggle/input/immagini-stable/kaggle/working/Immagini_stable/64x64'\n","gan_64x64_dir = '/kaggle/input/immagini-gan/kaggle/working/Immagini_gan/64x64'\n","\n","# Batch size\n","batch_size = 128\n","\n","# Dataset e DataLoader per le immagini COCO 64x64 (2000 immagini)\n","dataset_coco_64 = ImageDataset(coco_dir, 64)\n","dataset_coco_64 = Subset(dataset_coco_64, range(2000))\n","dataloader_coco_64 = DataLoader(dataset_coco_64, batch_size=batch_size, shuffle=False)\n","\n","# Dataset e DataLoader per le immagini Stable 64x64 (1000 immagini)\n","dataset_stable_64 = ImageDataset(stable_64x64_dir, 64)\n","dataset_stable_64 = Subset(dataset_stable_64, range(1000))\n","dataloader_stable_64 = DataLoader(dataset_stable_64, batch_size=batch_size, shuffle=False)\n","\n","# Dataset e DataLoader per le immagini GAN 64x64 (1000 immagini)\n","dataset_gan_64 = ImageDataset(gan_64x64_dir, 64)\n","dataset_gan_64 = Subset(dataset_gan_64, range(1000))\n","dataloader_gan_64 = DataLoader(dataset_gan_64, batch_size=batch_size, shuffle=False)\n","\n","# Risultati e directory per le metriche\n","metrics_file = '/kaggle/working/metrics_combined_64x64.txt'\n","\n","# Esegui il calcolo e salva i risultati\n","print(\"Processing and calculating metrics...\")\n","process_images(dataloader_coco_64, dataloader_stable_64, dataloader_gan_64, vgg16, svm_classifier, metrics_file)\n","\n","print(\"Processing complete.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","# Configurazione per l'uso di GPU o CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","ngpu = torch.cuda.device_count()\n","\n","# Carica il modello VGG16 pre-addestrato\n","vgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n","vgg16.classifier = vgg16.classifier[:-1]  # Rimuovi l'ultimo strato di classificazione\n","\n","# Se ci sono più GPU, utilizza DataParallel per parallelizzare il carico di lavoro\n","if ngpu > 1:\n","    vgg16 = torch.nn.DataParallel(vgg16)\n","vgg16.eval().to(device)  # Imposta il modello in modalità di valutazione e spostalo su GPU/CPU\n","\n","# Funzione per preprocessare e comprimere le immagini (64x64)\n","def preprocess_and_compress_image(image, size=64, compression_quality=100):  # Compressione, 100 = nessuna compressione\n","    \"\"\"\n","    Preprocessa e comprime un'immagine ridimensionandola a una dimensione specificata, poi la salva con un livello\n","    di compressione specifico e la ricarica per ulteriori operazioni.\n","\n","    Args:\n","    - image (PIL.Image): Immagine da preprocessare.\n","    - size (int): Dimensione a cui ridimensionare l'immagine (default 64).\n","    - compression_quality (int): Qualità della compressione JPEG (100 = nessuna compressione).\n","\n","    Returns:\n","    - Tensor: Immagine preprocessata e compressa.\n","    \"\"\"\n","    transform = transforms.Compose([\n","        transforms.Resize((size, size)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","    # Comprime l'immagine utilizzando il formato JPEG con la qualità specificata\n","    temp_img_path = '/kaggle/working/temp_image.jpg'\n","    image = image.resize((size, size))  # Ridimensiona l'immagine\n","    image.save(temp_img_path, format='JPEG', quality=compression_quality)  # Salva con la qualità specificata\n","\n","    # Ricarica l'immagine compressa e preprocessala\n","    compressed_image = Image.open(temp_img_path).convert('RGB')\n","    return transform(compressed_image)\n","\n","# Dataset personalizzato per caricare le immagini da una cartella e applicare la compressione\n","class ImageDataset(Dataset):\n","    \"\"\"\n","    Classe per creare un dataset personalizzato che carica immagini da una directory e applica\n","    la compressione e il preprocessing.\n","\n","    Args:\n","    - image_dir (str): Percorso alla directory delle immagini.\n","    - size (int): Dimensione a cui ridimensionare le immagini.\n","    - compression_quality (int): Qualità della compressione JPEG.\n","    \"\"\"\n","    def __init__(self, image_dir, size, compression_quality=100):\n","        self.image_dir = image_dir\n","        self.size = size\n","        self.compression_quality = compression_quality\n","        self.image_names = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]\n","\n","    def __len__(self):\n","        return len(self.image_names)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.image_dir, self.image_names[idx])\n","        image = Image.open(img_path).convert('RGB')\n","        image = preprocess_and_compress_image(image, self.size, self.compression_quality)\n","        return image, self.image_names[idx]\n","\n","# Funzione per estrarre le caratteristiche con VGG16\n","def extract_features(images, model):\n","    \"\"\"\n","    Estrae le caratteristiche dalle immagini utilizzando il modello VGG16.\n","\n","    Args:\n","    - images (Tensor): Batch di immagini preprocessate.\n","    - model (torch.nn.Module): Modello pre-addestrato per l'estrazione delle caratteristiche.\n","\n","    Returns:\n","    - np.ndarray: Caratteristiche estratte dalle immagini.\n","    \"\"\"\n","    images = images.to(device)\n","    with torch.no_grad():  # Disabilita il calcolo dei gradienti per risparmiare memoria\n","        features = model(images)\n","    return features.flatten(1).cpu().numpy()\n","\n","# Funzione per calcolare e salvare le metriche\n","def calculate_metrics(y_true, y_pred, save_path):\n","    \"\"\"\n","    Calcola l'accuratezza, precisione, richiamo e F1 score e salva i risultati in un file di testo.\n","\n","    Args:\n","    - y_true (list): Lista dei valori reali (1 per reale, 0 per generato).\n","    - y_pred (list): Lista delle predizioni effettuate dal modello.\n","    - save_path (str): Percorso del file dove salvare i risultati.\n","    \"\"\"\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","\n","    # Salva i risultati in un file di testo\n","    with open(save_path, 'w') as f:\n","        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n","        f.write(f\"Precision: {precision:.4f}\\n\")\n","        f.write(f\"Recall: {recall:.4f}\\n\")\n","        f.write(f\"F1 Score: {f1:.4f}\\n\")\n","\n","    print(f\"Metrics saved to {save_path}\")\n","\n","# Funzione per processare le immagini in batch\n","def process_images_in_batches(dataloader_real, dataloader_generated, model, svm, result_file):\n","    \"\"\"\n","    Processa le immagini reali e generate in batch, estrae le caratteristiche e confronta le predizioni\n","    utilizzando un classificatore SVM.\n","\n","    Args:\n","    - dataloader_real (DataLoader): Dataloader per le immagini reali.\n","    - dataloader_generated (DataLoader): Dataloader per le immagini generate.\n","    - model (torch.nn.Module): Modello pre-addestrato per l'estrazione delle caratteristiche.\n","    - svm (sklearn.svm.SVC): Classificatore SVM per confrontare le caratteristiche.\n","    - result_file (str): Percorso del file dove salvare i risultati delle metriche.\n","    \"\"\"\n","    y_true = []\n","    y_pred = []\n","\n","    for real_batch, _ in dataloader_real:\n","        real_batch = real_batch.to(device)\n","        real_features = extract_features(real_batch, model)\n","\n","        for gen_batch, _ in dataloader_generated:\n","            gen_batch = gen_batch.to(device)\n","            gen_features = extract_features(gen_batch, model)\n","\n","            combined_features = torch.cat([torch.tensor(real_features), torch.tensor(gen_features)], dim=0).numpy()\n","\n","            predictions = svm.predict(combined_features)\n","\n","            y_true.extend([1] * len(real_batch) + [0] * len(gen_batch))\n","            y_pred.extend(predictions)\n","\n","            # Libera la memoria GPU dopo ogni batch\n","            torch.cuda.empty_cache()\n","\n","    calculate_metrics(y_true, y_pred, result_file)\n","\n","# Percorsi per i checkpoint SVM\n","svm_checkpoint_64 = '/kaggle/input/checkpoint-classificatore-6464/svm_classifier_64.pkl'\n","\n","# Percorsi per le immagini ridimensionate\n","coco_dir = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n","stable_64x64_dir = '/kaggle/input/immagini-stable/kaggle/working/Immagini_stable/64x64'\n","gan_64x64_dir = '/kaggle/input/immagini-gan/kaggle/working/Immagini_gan/64x64'\n","\n","# Batch size\n","batch_size = 128\n","\n","# Livelli di compressione\n","compression_levels = {\n","    '40': 60,  # 40% di compressione (60% qualità)\n","    '60': 40,  # 60% di compressione (40% qualità)\n","    '80': 20   # 80% di compressione (20% qualità)\n","}\n","\n","# Dataset e DataLoader per le immagini COCO 64x64 (limite a 1000 immagini)\n","dataset_coco_64 = ImageDataset(coco_dir, 64, compression_quality=100)  # Nessuna compressione per COCO\n","dataset_coco_64 = Subset(dataset_coco_64, range(1000))\n","dataloader_coco_64 = DataLoader(dataset_coco_64, batch_size=batch_size, shuffle=False)\n","\n","# Carica il classificatore SVM per 64x64\n","svm_64 = joblib.load(svm_checkpoint_64)\n","\n","# Itera sui diversi livelli di compressione\n","for level, quality in compression_levels.items():\n","    # Dataset e DataLoader per il dataset misto GAN e Stable (1000 immagini da ciascuno)\n","    dataset_stable_64 = ImageDataset(stable_64x64_dir, 64, compression_quality=quality)\n","    dataset_stable_64 = Subset(dataset_stable_64, range(1000))\n","\n","    dataset_gan_64 = ImageDataset(gan_64x64_dir, 64, compression_quality=quality)\n","    dataset_gan_64 = Subset(dataset_gan_64, range(1000))\n","\n","    # Unisci i dataset GAN e Stable in un unico DataLoader\n","    combined_dataset = dataset_stable_64 + dataset_gan_64\n","    dataloader_combined = DataLoader(combined_dataset, batch_size=batch_size, shuffle=False)\n","\n","    # Confronto tra COCO e il dataset misto (64x64 con compressione)\n","    print(f\"Processing COCO vs GAN + Stable Diffusion (64x64, compressione {level}%)...\")\n","    result_file = f'/kaggle/working/metrics_coco_combined_6464_compression_{level}.txt'\n","    process_images_in_batches(dataloader_coco_64, dataloader_combined, vgg16, svm_64, result_file)\n","\n","print(\"Processing complete.\")\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":857191,"sourceId":1462296,"sourceType":"datasetVersion"},{"datasetId":5851363,"sourceId":9593156,"sourceType":"datasetVersion"},{"datasetId":5851613,"sourceId":9593490,"sourceType":"datasetVersion"},{"datasetId":5851619,"sourceId":9593497,"sourceType":"datasetVersion"},{"datasetId":5852871,"sourceId":9595088,"sourceType":"datasetVersion"},{"datasetId":5852895,"sourceId":9595118,"sourceType":"datasetVersion"}],"dockerImageVersionId":30787,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
