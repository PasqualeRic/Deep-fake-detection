{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":9593156,"sourceType":"datasetVersion","datasetId":5851363},{"sourceId":9593490,"sourceType":"datasetVersion","datasetId":5851613},{"sourceId":9593497,"sourceType":"datasetVersion","datasetId":5851619},{"sourceId":9595088,"sourceType":"datasetVersion","datasetId":5852871},{"sourceId":9595118,"sourceType":"datasetVersion","datasetId":5852895},{"sourceId":9619602,"sourceType":"datasetVersion","datasetId":5870973}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install diffusers\n!pip install transformers\n!pip install accelerate\n!pip install ftfy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-14T19:07:00.311921Z","iopub.execute_input":"2024-10-14T19:07:00.312221Z","iopub.status.idle":"2024-10-14T19:07:50.873986Z","shell.execute_reply.started":"2024-10-14T19:07:00.312174Z","shell.execute_reply":"2024-10-14T19:07:50.872883Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting diffusers\n  Downloading diffusers-0.30.3-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from diffusers) (7.0.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from diffusers) (3.15.1)\nRequirement already satisfied: huggingface-hub>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.25.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from diffusers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from diffusers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from diffusers) (2.32.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from diffusers) (0.4.5)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from diffusers) (10.3.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.2->diffusers) (4.12.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->diffusers) (3.19.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->diffusers) (2024.8.30)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.23.2->diffusers) (3.1.2)\nDownloading diffusers-0.30.3-py3-none-any.whl (2.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: diffusers\nSuccessfully installed diffusers-0.30.3\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.25.1)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nCollecting ftfy\n  Downloading ftfy-6.3.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.13)\nDownloading ftfy-6.3.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ftfy\nSuccessfully installed ftfy-6.3.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import models, transforms, utils as vutils\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport joblib\nfrom torch.utils.data import DataLoader, Dataset, Subset\nfrom diffusers import DiffusionPipeline\nimport matplotlib.pyplot as plt\nimport json  \n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T19:07:50.875775Z","iopub.execute_input":"2024-10-14T19:07:50.876069Z","iopub.status.idle":"2024-10-14T19:08:11.188999Z","shell.execute_reply.started":"2024-10-14T19:07:50.876034Z","shell.execute_reply":"2024-10-14T19:08:11.188224Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032744f0225845fba99bd1562b970ce5"}},"metadata":{}}]},{"cell_type":"code","source":"\n\n# Verifica delle GPU disponibili\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndtype = torch.float16 if device == \"cuda\" else torch.float32\n\n# Caricamento della pipeline Stable Diffusion\npipeline = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=dtype)\npipeline.to(device)\n\n# Percorso al file JSON delle annotazioni COCO (modifica il percorso in base alla tua configurazione)\ncoco_annotation_file = '/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_train2017.json'\n\n# Funzione per leggere il file JSON delle annotazioni\ndef load_coco_annotations(json_file):\n    \"\"\"\n    Carica le annotazioni dal file JSON del dataset COCO.\n    \n    Args:\n    - json_file (str): Il percorso del file JSON con le annotazioni COCO.\n\n    Returns:\n    - dict: Un dizionario con i dati delle annotazioni COCO.\n    \"\"\"\n    with open(json_file, 'r') as f:\n        data = json.load(f)\n    return data\n\n# Carica le annotazioni COCO\ncoco_data = load_coco_annotations(coco_annotation_file)\n\n# Mappa degli ID delle categorie alle descrizioni delle categorie\ncategories = {cat['id']: cat['name'] for cat in coco_data['categories']}\n\n# Creazione delle directory per salvare le immagini 64x64, 256x256 e originali localmente su Kaggle\nbase_dir = '/kaggle/working/Immagini_stable/'\ndir_64x64 = os.path.join(base_dir, '64x64')\ndir_256x256 = os.path.join(base_dir, '256x256')\ndir_original = os.path.join(base_dir, 'original')\n\n# Creazione delle directory se non esistono già\nos.makedirs(dir_64x64, exist_ok=True)\nos.makedirs(dir_256x256, exist_ok=True)\nos.makedirs(dir_original, exist_ok=True)\n\n# Estrazione delle annotazioni per le immagini\nimage_annotations = {}\nfor annotation in coco_data['annotations']:\n    img_id = annotation['image_id']\n    category_name = categories[annotation['category_id']]\n    \n    if img_id in image_annotations:\n        image_annotations[img_id].append(category_name)\n    else:\n        image_annotations[img_id] = [category_name]\n\n# Selezione delle seconde 2000 immagini\nsample_ids = list(image_annotations.keys())[2000:4000]  # Seleziona le immagini dal 2001° al 4000°\n\n# Funzione per generare immagini a partire da un prompt\ndef generate_image(prompt):\n    \"\"\"\n    Genera un'immagine utilizzando la pipeline Stable Diffusion a partire da un prompt.\n    \n    Args:\n    - prompt (str): Il prompt testuale per generare l'immagine.\n\n    Returns:\n    - image: L'immagine generata come oggetto PIL.Image.\n    \"\"\"\n    image = pipeline(prompt, num_inference_steps=8).images[0]\n    return image.convert(\"RGB\")\n\n# Iterazione per generare le seconde 2000 immagini e salvarle\nfor i, img_id in enumerate(sample_ids, start=1):  # Inizia la numerazione da 1\n    try:\n        # Crea il prompt dalle categorie associate a questa immagine\n        prompt = \", \".join(image_annotations[img_id])\n\n        # Genera immagine fake usando il prompt\n        image_fake = generate_image(prompt)\n\n        # Ridimensiona l'immagine a 64x64 e 256x256\n        image_64x64 = image_fake.resize((64, 64))\n        image_256x256 = image_fake.resize((256, 256))\n\n        # Percorsi per salvare le immagini nelle sottocartelle locali\n        save_path_64 = f'{dir_64x64}/fake_image_{i}.png'\n        save_path_256 = f'{dir_256x256}/fake_image_{i}.png'\n        save_path_original = f'{dir_original}/image_{i}.png'  # Nome immagine originale con numerazione da 1\n\n        # Salva le immagini nelle rispettive directory\n        image_64x64.save(save_path_64)\n        image_256x256.save(save_path_256)\n        image_fake.save(save_path_original)  # Salva l'immagine originale come image_{i}.png\n\n        print(f\"Immagine {i}/2000 generata e salvata in {save_path_64}, {save_path_256}, e {save_path_original}\")\n\n    except Exception as e:\n        print(f\"Errore nella generazione dell'immagine ID {img_id}: {e}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T19:04:59.623168Z","iopub.status.idle":"2024-10-14T19:04:59.623679Z","shell.execute_reply.started":"2024-10-14T19:04:59.623397Z","shell.execute_reply":"2024-10-14T19:04:59.623422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    command = f\"zip {zip_name} {path} -r\"\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    display(FileLink(f'{download_file_name}.zip'))\ndownload_file('/kaggle/working/Immagini_stable', 'out')\n#download_file('/kaggle/working/Immagini_gan', 'out1')\n#download_file('/kaggle/working/IS_FIS_results', 'out2')","metadata":{"execution":{"iopub.status.busy":"2024-10-14T19:04:59.625033Z","iopub.status.idle":"2024-10-14T19:04:59.625507Z","shell.execute_reply.started":"2024-10-14T19:04:59.625258Z","shell.execute_reply":"2024-10-14T19:04:59.625283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Percorso della cartella da eliminare\nbase_dir = '/kaggle/working/metrics_combined_64x64_compression_60.txt'\n\n# Controlla se la cartella esiste, quindi eliminala insieme a tutti i suoi contenuti\nif os.path.exists(base_dir):\n    shutil.rmtree(base_dir)\n    print(f\"Tutto il contenuto della cartella {base_dir} è stato eliminato.\")\nelse:\n    print(f\"La cartella {base_dir} non esiste.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T19:04:59.626874Z","iopub.status.idle":"2024-10-14T19:04:59.627335Z","shell.execute_reply.started":"2024-10-14T19:04:59.627094Z","shell.execute_reply":"2024-10-14T19:04:59.627118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Iperparametri per il Generatore\nimage_size = 64  # Dimensione delle immagini generate\nnz = 100  # Dimensione del vettore latente\nngf = 64  # Numero di feature maps nel generatore\nnc = 3  # Numero di canali nelle immagini di allenamento (RGB)\nngpu = torch.cuda.device_count()  # Rileva automaticamente il numero di GPU disponibili\n\n# Definizione del modello Generator\nclass Generator(nn.Module):\n    \"\"\"\n    Una classe che rappresenta il modello del Generatore per la generazione di immagini.\n    Il modello utilizza una serie di layer ConvTranspose2d per upscalare il vettore latente \n    fino a ottenere la dimensione dell'immagine desiderata.\n\n    Args:\n    - ngpu (int): Numero di GPU disponibili per il calcolo parallelo.\n    \"\"\"\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),  # Layer 1\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),  # Layer 2\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),  # Layer 3\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),  # Layer 4\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),  # Layer finale\n            nn.Tanh()  # Output scalato tra -1 e 1\n        )\n\n    def forward(self, input):\n        \"\"\"\n        Passaggio forward del Generatore. Riceve un vettore latente e produce un'immagine.\n        \n        Args:\n        - input (Tensor): Un batch di vettori latenti con forma [batch_size, nz, 1, 1].\n\n        Returns:\n        - Tensor: Immagini generate.\n        \"\"\"\n        return self.main(input)\n\n# Inizializzazione del generatore\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nnetG = Generator(ngpu=ngpu)  # Creazione di un'istanza del modello Generator\n\n# Usa DataParallel se sono disponibili più GPU\nif ngpu > 1:\n    netG = nn.DataParallel(netG, list(range(ngpu)))\n\nnetG = netG.to(device)  # Sposta il modello su GPU (o CPU se non disponibile)\n\n# Carica i pesi del generatore da un checkpoint\ncheckpoint_path_generator = '/kaggle/input/checkpoint-generatore/checkpoint_generator.pth'\nstate_dict = torch.load(checkpoint_path_generator, map_location=device)\n\n# Controlla se lo state_dict proviene da un modello DataParallel e modifica se necessario\nnew_state_dict = {}\nfor key, value in state_dict.items():\n    if ngpu > 1 and not key.startswith(\"module.\"):\n        new_state_dict[\"module.\" + key] = value  # Aggiungi 'module.' come prefisso se si usa DataParallel\n    elif ngpu == 1 and key.startswith(\"module.\"):\n        new_state_dict[key[len(\"module.\"):]] = value  # Rimuovi 'module.' se non si usa DataParallel\n    else:\n        new_state_dict[key] = value\n\n# Carica il nuovo state dict modificato nel modello\nnetG.load_state_dict(new_state_dict)\n\n# Imposta il generatore in modalità di valutazione\nnetG.eval()\n\n# Genera vettori latenti (rumore casuale)\nfixed_noise = torch.randn(2000, nz, 1, 1, device=device)  # Genera 2000 vettori di rumore casuale\n\n# Genera 2000 immagini fake dal generatore\nwith torch.no_grad(): \n    fake_images = netG(fixed_noise).detach().cpu()  # Genera immagini e spostale su CPU\n\n# Imposta la directory per salvare le immagini generate\nsave_directory = '/kaggle/working/Immagini_gan/'\ndir_64x64 = os.path.join(save_directory, '64x64')\ndir_256x256 = os.path.join(save_directory, '256x256')\n\n# Crea le directory se non esistono\nos.makedirs(dir_64x64, exist_ok=True)\nos.makedirs(dir_256x256, exist_ok=True)\n\n# Salva ogni immagine generata in versioni 64x64 e 256x256\nfor i in range(fake_images.size(0)):\n    image_64_path = os.path.join(dir_64x64, f'generated_image_{i+1}.png')\n    image_256_path = os.path.join(dir_256x256, f'generated_image_{i+1}.png')\n\n    # Salva la versione 64x64\n    vutils.save_image(fake_images[i], image_64_path, normalize=True)\n\n    # Ridimensiona l'immagine a 256x256 e salvala\n    image_256 = F.interpolate(fake_images[i].unsqueeze(0), size=(256, 256)).squeeze(0)\n    vutils.save_image(image_256, image_256_path, normalize=True)\n\n    print(f\"Immagini salvate: {image_64_path}, {image_256_path}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T19:04:59.629164Z","iopub.status.idle":"2024-10-14T19:04:59.629668Z","shell.execute_reply.started":"2024-10-14T19:04:59.629393Z","shell.execute_reply":"2024-10-14T19:04:59.629418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Percorsi per le immagini GAN e Stable Diffusion (tutte considerate \"generate\")\ngan_images_dir_64 = '/kaggle/working/Immagini_gan/64x64'\ngan_images_dir_256 = '/kaggle/working/Immagini_gan/256x256'\nstable_images_dir_64 = '/kaggle/working/Immagini_stable/64x64'\nstable_images_dir_256 = '/kaggle/working/Immagini_stable/256x256'\n\n# Percorsi per salvare i risultati IS e FIS\nresults_dir = '/kaggle/working/IS_FIS_results'\nos.makedirs(results_dir, exist_ok=True)\n\n# Dispositivo GPU/CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Percorso del dataset COCO (sostituisci con il percorso corretto)\ncoco_annotation_file = '/kaggle/input/coco-2017-dataset/coco2017/annotations/instances_train2017.json'\ncoco_images_dir = '/kaggle/input/coco-2017-dataset/coco2017/train2017'  # Directory delle immagini COCO\n\n# Funzione per caricare immagini dal dataset COCO\ndef load_coco_images(coco_annotation_file, coco_images_dir, start_idx, end_idx, image_size):\n    \"\"\"\n    Carica un insieme di immagini dal dataset COCO tra gli indici specificati, ridimensionandole a una data dimensione.\n    \n    Args:\n    - coco_annotation_file (str): Percorso al file JSON delle annotazioni COCO.\n    - coco_images_dir (str): Directory delle immagini COCO.\n    - start_idx (int): Indice iniziale delle immagini da caricare.\n    - end_idx (int): Indice finale delle immagini da caricare.\n    - image_size (int): Dimensione a cui ridimensionare le immagini.\n\n    Returns:\n    - list: Lista di immagini caricate e ridimensionate.\n    \"\"\"\n    with open(coco_annotation_file, 'r') as f:\n        coco_data = json.load(f)\n    \n    image_ids = [img['file_name'] for img in coco_data['images'][start_idx:end_idx]][:1000]\n    images = []\n    \n    for img_id in image_ids:\n        img_path = os.path.join(coco_images_dir, img_id)\n        img = Image.open(img_path).convert(\"RGB\")\n        img = img.resize((image_size, image_size))\n        images.append(img)\n    \n    return images\n\n# Funzione per caricare immagini generate da una directory (limita a 1000 immagini)\ndef load_images_from_dir(directory, image_size, limit=1000):\n    \"\"\"\n    Carica le immagini generate da una directory e le ridimensiona a una data dimensione.\n    \n    Args:\n    - directory (str): Directory da cui caricare le immagini generate.\n    - image_size (int): Dimensione a cui ridimensionare le immagini.\n    - limit (int): Numero massimo di immagini da caricare.\n\n    Returns:\n    - list: Lista di immagini caricate e ridimensionate.\n    \"\"\"\n    images = []\n    for filename in os.listdir(directory)[:limit]:\n        if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n            img_path = os.path.join(directory, filename)\n            img = Image.open(img_path).convert(\"RGB\")\n            img = img.resize((image_size, image_size))\n            images.append(img)\n    return images\n\n# Preprocessing per InceptionV3 (resize to 299x299) e ResNet (resize alle dimensioni originali)\ndef preprocess_images(images, image_size, for_inception=False):\n    \"\"\"\n    Preprocessa un insieme di immagini per essere utilizzate in un modello di deep learning.\n    \n    Args:\n    - images (list): Lista di immagini da preprocessare.\n    - image_size (int): Dimensione a cui ridimensionare le immagini.\n    - for_inception (bool): Se True, ridimensiona le immagini a 299x299 per InceptionV3.\n\n    Returns:\n    - Tensor: Batch di immagini preprocessate.\n    \"\"\"\n    if for_inception:\n        preprocess = transforms.Compose([\n            transforms.Resize(299),\n            transforms.CenterCrop(299),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ])\n    else:\n        preprocess = transforms.Compose([\n            transforms.Resize((image_size, image_size)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ])\n    \n    return torch.stack([preprocess(img) for img in images]).to(device)\n\n# Funzione per calcolare l'Inception Score (IS)\ndef calculate_inception_score(images, splits=10):\n    \"\"\"\n    Calcola l'Inception Score per un insieme di immagini.\n    \n    Args:\n    - images (Tensor): Batch di immagini preprocessate.\n    - splits (int): Numero di suddivisioni per calcolare IS.\n\n    Returns:\n    - tuple: Media e deviazione standard dell'Inception Score.\n    \"\"\"\n    inception_model = models.inception_v3(pretrained=True, transform_input=False).to(device)\n    inception_model.eval()\n\n    def get_pred(x):\n        x = inception_model(x)\n        return torch.nn.functional.softmax(x, dim=1).data.cpu().numpy()\n\n    N = len(images)\n    batch_size = 32\n    preds = np.zeros((N, 1000))\n\n    for i in range(0, N, batch_size):\n        batch_images = images[i:i + batch_size]\n        preds[i:i + batch_size] = get_pred(batch_images)\n\n    scores = []\n    for i in range(splits):\n        part = preds[i * (N // splits): (i + 1) * (N // splits), :]\n        py = np.mean(part, axis=0)\n        kl_div = part * (np.log(part) - np.log(py))\n        kl_div = np.sum(kl_div, axis=1)\n        scores.append(np.exp(np.mean(kl_div)))\n\n    return np.mean(scores), np.std(scores)\n\n# Funzione per estrarre le feature dalle immagini usando ResNet18\ndef extract_features(images, model):\n    \"\"\"\n    Estrae le feature da un batch di immagini utilizzando un modello pre-addestrato.\n    \n    Args:\n    - images (Tensor): Batch di immagini preprocessate.\n    - model (torch.nn.Module): Modello pre-addestrato per l'estrazione delle feature.\n\n    Returns:\n    - Tensor: Feature estratte dalle immagini.\n    \"\"\"\n    with torch.no_grad():\n        features = model(images)\n    return features\n\n# Funzione per calcolare il Feature Importance Score (FIS)\ndef calculate_fis(real_images, generated_images, model, batch_size=32):\n    \"\"\"\n    Calcola il Feature Importance Score (FIS) tra immagini reali e generate.\n    \n    Args:\n    - real_images (Tensor): Batch di immagini reali.\n    - generated_images (Tensor): Batch di immagini generate.\n    - model (torch.nn.Module): Modello pre-addestrato per l'estrazione delle feature.\n    - batch_size (int): Dimensione della batch per l'elaborazione.\n\n    Returns:\n    - float: Punteggio medio del FIS.\n    \"\"\"\n    total_distance = 0.0\n    num_batches = 0\n\n    for i in range(0, len(real_images), batch_size):\n        real_batch = real_images[i:i + batch_size]\n        generated_batch = generated_images[i:i + batch_size]\n        \n        real_features = extract_features(real_batch, model)\n        generated_features = extract_features(generated_batch, model)\n        \n        distance = torch.norm(real_features - generated_features, dim=1).mean().item()\n        total_distance += distance\n        num_batches += 1\n\n    return total_distance / num_batches\n\n# Funzione per salvare il grafico FIS e i risultati finali\ndef save_fis_results(fis_score, fis_txt_file, fis_plot_file):\n    \"\"\"\n    Salva i risultati del Feature Importance Score (FIS) in un file di testo e un grafico.\n    \n    Args:\n    - fis_score (float): Valore del punteggio FIS.\n    - fis_txt_file (str): Percorso del file di testo per salvare il punteggio.\n    - fis_plot_file (str): Percorso del file per salvare il grafico FIS.\n    \"\"\"\n    with open(fis_txt_file, 'w') as f:\n        f.write(f\"Feature Importance Score (FIS): {fis_score:.4f}\\n\")\n\n    plt.figure(figsize=(6, 4))\n    plt.plot([1], [fis_score], marker='o', linestyle='-', color='b')\n    plt.title(\"Feature Importance Score (FIS) Finale\")\n    plt.xlabel(\"Valore Finale\")\n    plt.ylabel(\"Feature Importance Score (FIS)\")\n    plt.grid(True)\n\n    plt.savefig(fis_plot_file)\n    plt.close()\n\n# Funzione principale per calcolare IS e FIS\ndef calculate_is_and_fis(image_size, gan_dir, stable_dir, results_prefix, batch_size=32):\n    \"\"\"\n    Calcola l'Inception Score (IS) e il Feature Importance Score (FIS) per immagini reali e generate.\n    \n    Args:\n    - image_size (int): Dimensione delle immagini da utilizzare.\n    - gan_dir (str): Directory delle immagini GAN generate.\n    - stable_dir (str): Directory delle immagini Stable Diffusion generate.\n    - results_prefix (str): Prefisso per i file di risultati.\n    - batch_size (int): Dimensione della batch per l'elaborazione.\n\n    Returns:\n    - None\n    \"\"\"\n    # Caricamento delle immagini reali dal dataset COCO\n    real_images = load_coco_images(coco_annotation_file, coco_images_dir, 2000, 4000, image_size)\n    real_images_tensor = preprocess_images(real_images[:1000], image_size)  # Usa solo 1000 immagini reali\n\n    # Caricamento delle immagini generate (GAN e Stable)\n    gan_images = load_images_from_dir(gan_dir, image_size, limit=1000)\n    stable_images = load_images_from_dir(stable_dir, image_size, limit=1000)\n    \n    # Unisci tutte le immagini generate (GAN + Stable)\n    generated_images = gan_images + stable_images\n    generated_images_tensor = preprocess_images(generated_images[:1000], image_size)  # Usa solo 1000 immagini generate\n\n    # Inception Score per immagini generate\n    generated_images_tensor_299 = preprocess_images(generated_images[:1000], image_size, for_inception=True)  # Resize a 299x299\n    mean_is_gan, std_is_gan = calculate_inception_score(generated_images_tensor_299)\n    print(f\"Inception Score Generated ({image_size}x{image_size}): Mean = {mean_is_gan}, Std = {std_is_gan}\")\n\n    # Salva i risultati IS in un file di testo\n    is_results_file = os.path.join(results_dir, f'{results_prefix}_inception_score_results.txt')\n    with open(is_results_file, 'w') as f:\n        f.write(f\"Inception Score ({image_size}x{image_size}): Mean = {mean_is_gan}, Std = {std_is_gan}\\n\")\n\n    # Caricamento del modello ResNet18 per calcolare FIS\n    resnet_model = models.resnet18(pretrained=True).eval().to(device)\n    resnet_model = nn.Sequential(*list(resnet_model.children())[:-1])  # Rimuovi il layer di classificazione\n\n    # Feature Importance Score (FIS) per immagini generate (considerando tutte le batch)\n    fis_gan = calculate_fis(real_images_tensor, generated_images_tensor, resnet_model, batch_size=batch_size)\n    print(f\"Feature Importance Score Generated ({image_size}x{image_size}): {fis_gan}\")\n\n    # Salva i risultati FIS\n    fis_results_file = os.path.join(results_dir, f'{results_prefix}_fis_results.txt')\n    fis_plot_file = os.path.join(results_dir, f'{results_prefix}_fis_plot.png')\n    save_fis_results(fis_gan, fis_results_file, fis_plot_file)\n\n# Calcolo per 64x64 (GAN e Stable Diffusion come generate, COCO come reali)\ncalculate_is_and_fis(64, gan_images_dir_64, stable_images_dir_64, \"64x64\", batch_size=16)\n\n# Calcolo per 256x256 (GAN e Stable Diffusion come generate, COCO come reali)\ncalculate_is_and_fis(256, gan_images_dir_256, stable_images_dir_256, \"256x256\", batch_size=8)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T19:04:59.631054Z","iopub.status.idle":"2024-10-14T19:04:59.631538Z","shell.execute_reply.started":"2024-10-14T19:04:59.631276Z","shell.execute_reply":"2024-10-14T19:04:59.631302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration to use GPU or CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nngpu = torch.cuda.device_count()\n\n# Load pre-trained VGG16 model\nvgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\nvgg16.classifier = vgg16.classifier[:-1]  # Remove the final classification layer\n\n# If multiple GPUs are available, use DataParallel\nif ngpu > 1:\n    vgg16 = torch.nn.DataParallel(vgg16)\n\nvgg16.eval().to(device)  # Set the model to evaluation mode and move it to GPU/CPU\n\n# Function to preprocess images (64x64 or 256x256)\ndef preprocess_image(image, size=64):\n    \"\"\"\n    Preprocess an image by resizing and normalizing it.\n    \n    Args:\n    - image (PIL.Image): Image to preprocess.\n    - size (int): The size to resize the image to (default 64).\n    \n    Returns:\n    - Tensor: Preprocessed image as a Tensor.\n    \"\"\"\n    transform = transforms.Compose([\n        transforms.Resize((size, size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    return transform(image)\n\n# Custom dataset class to load images from a folder\nclass ImageDataset(Dataset):\n    \"\"\"\n    Custom dataset class to load images from a directory and preprocess them.\n    \n    Args:\n    - image_dir (str): Path to the directory containing images.\n    - size (int): The size to resize the images to.\n    \"\"\"\n    def __init__(self, image_dir, size=None):\n        self.image_dir = image_dir\n        self.size = size\n        self.image_names = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.image_names[idx])\n        image = Image.open(img_path).convert('RGB')\n        if self.size:\n            image = preprocess_image(image, self.size)\n        return image, self.image_names[idx]\n\n# Function to extract features using VGG16\ndef extract_features(images, model):\n    \"\"\"\n    Extract features from images using the VGG16 model.\n    \n    Args:\n    - images (Tensor): Batch of preprocessed images.\n    - model (torch.nn.Module): Pre-trained model (VGG16) for feature extraction.\n    \n    Returns:\n    - np.ndarray: Extracted features from the images.\n    \"\"\"\n    images = images.to(device)\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        features = model(images)\n    return features.flatten(1).cpu().numpy()\n\n# Function to calculate and save metrics\ndef calculate_metrics(y_true, y_pred, save_path):\n    \"\"\"\n    Calculate accuracy, precision, recall, and F1 score, then save the results to a file.\n    \n    Args:\n    - y_true (list): List of true (real) values.\n    - y_pred (list): List of predictions (generated by the model).\n    - save_path (str): Path to the file where the results will be saved.\n    \"\"\"\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n\n    # Save the results to a text file\n    with open(save_path, 'w') as f:\n        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n        f.write(f\"Precision: {precision:.4f}\\n\")\n        f.write(f\"Recall: {recall:.4f}\\n\")\n        f.write(f\"F1 Score: {f1:.4f}\\n\")\n\n    print(f\"Metrics saved to {save_path}\")\n\n# Function to process images in batches\ndef process_images_in_batches(dataloader_real, dataloader_generated, model, svm, result_file):\n    \"\"\"\n    Process real and generated images in batches, extract features, and compare predictions\n    using an SVM classifier.\n\n    Args:\n    - dataloader_real (DataLoader): Dataloader for real images.\n    - dataloader_generated (DataLoader): Dataloader for generated images.\n    - model (torch.nn.Module): Pre-trained model for feature extraction.\n    - svm (sklearn.svm.SVC): SVM classifier to compare the features.\n    - result_file (str): Path to the file where the metric results will be saved.\n    \"\"\"\n    y_true = []\n    y_pred = []\n\n    for real_batch, _ in dataloader_real:\n        real_batch = real_batch.to(device)\n        real_features = extract_features(real_batch, model)\n\n        for gen_batch, _ in dataloader_generated:\n            gen_batch = gen_batch.to(device)\n            gen_features = extract_features(gen_batch, model)\n\n            # Combine real and generated features\n            combined_features = torch.cat([torch.tensor(real_features), torch.tensor(gen_features)], dim=0).numpy()\n\n            # Predictions using the SVM classifier\n            predictions = svm.predict(combined_features)\n\n            # Add true values (1 for real, 0 for generated) and predictions\n            y_true.extend([1] * len(real_batch) + [0] * len(gen_batch))\n            y_pred.extend(predictions)\n\n            # Free GPU memory after each batch\n            torch.cuda.empty_cache()\n\n    calculate_metrics(y_true, y_pred, result_file)\n\n# Paths for the SVM checkpoints\nsvm_checkpoint_64 = '/kaggle/input/checkpoint-classificatore-6464/svm_classifier_64.pkl'  # GAN 64x64 vs Stable 64x64\nsvm_checkpoint_256 = '/kaggle/input/checkpoint-classificatore-256256/svm_classifier_256.pkl'  # GAN 256x256 vs Stable 256x256\nsvm_checkpoint_mixed = '/kaggle/input/checkpoint-classificatore/svm_classifier-2.pkl'  # Mixed classifier for different resolutions\n\n# Paths for the resized images\ncoco_dir = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\nstable_64x64_dir = '/kaggle/input/immagini-gan/kaggle/working/Immagini_gan/64x64'\ngan_64x64_dir = '/kaggle/input/immagini-gan/kaggle/working/Immagini_gan/64x64'\nstable_256x256_dir = '/kaggle/input/immagini-stable/kaggle/working/Immagini_stable/256x256'\ngan_256x256_dir = '/kaggle/input/immagini-gan/kaggle/working/Immagini_gan/256x256'\n\n# Reduced batch size to limit GPU memory usage\nbatch_size = 128  # Reduced to prevent memory errors\n\n# Dataset and DataLoader for COCO images (no resize)\ndataset_coco_original = ImageDataset(coco_dir, size=None)\ndataloader_coco_original = DataLoader(dataset_coco_original, batch_size=batch_size, shuffle=False)\n\n# Dataset and DataLoader for COCO images 64x64\ndataset_coco_64 = ImageDataset(coco_dir, size=64)\ndataset_coco_64 = Subset(dataset_coco_64, range(2000))\ndataloader_coco_64 = DataLoader(dataset_coco_64, batch_size=batch_size, shuffle=False)\n\n# Dataset and DataLoader for COCO images 256x256\ndataset_coco_256 = ImageDataset(coco_dir, size=256)\ndataset_coco_256 = Subset(dataset_coco_256, range(2000))\ndataloader_coco_256 = DataLoader(dataset_coco_256, batch_size=batch_size, shuffle=False)\n\n# Dataset and DataLoader for Stable Diffusion 64x64 images (limited to 1000 images)\ndataset_stable_64 = ImageDataset(stable_64x64_dir, size=64)\ndataset_stable_64 = Subset(dataset_stable_64, range(2000))\ndataloader_stable_64 = DataLoader(dataset_stable_64, batch_size=batch_size, shuffle=False)\n\n# Dataset and DataLoader for GAN 64x64 images (limited to 1000 images)\ndataset_gan_64 = ImageDataset(gan_64x64_dir, size=64)\ndataset_gan_64 = Subset(dataset_gan_64, range(2000))\ndataloader_gan_64 = DataLoader(dataset_gan_64, batch_size=batch_size, shuffle=False)\n\n# Dataset and DataLoader for Stable Diffusion 256x256 images (limited to 1000 images)\ndataset_stable_256 = ImageDataset(stable_256x256_dir, size=256)\ndataset_stable_256 = Subset(dataset_stable_256, range(1000))\ndataloader_stable_256 = DataLoader(dataset_stable_256, batch_size=batch_size, shuffle=False)\n\n# Dataset and DataLoader for GAN 256x256 images (limited to 1000 images)\ndataset_gan_256 = ImageDataset(gan_256x256_dir, size=256)\ndataset_gan_256 = Subset(dataset_gan_256, range(1000))\ndataloader_gan_256 = DataLoader(dataset_gan_256, batch_size=batch_size, shuffle=False)\n\n# Load the SVM classifiers\nsvm_64 = joblib.load(svm_checkpoint_64)\nsvm_256 = joblib.load(svm_checkpoint_256)\nsvm_mixed = joblib.load(svm_checkpoint_mixed)\n\n# Comparison between COCO and GAN 64x64\nprint(\"Processing COCO vs GAN 64x64...\")\nprocess_images_in_batches(dataloader_coco_64, dataloader_gan_64, vgg16, svm_64, '/kaggle/working/metrics_coco_gan_6464.txt')\n\n# Comparison between COCO and GAN 256x256\nprint(\"Processing COCO vs GAN 256x256...\")\nprocess_images_in_batches(dataloader_coco_256, dataloader_gan_256, vgg16, svm_256, '/kaggle/working/metrics_coco_gan_256256.txt')\n\n# Comparison between COCO and Stable Diffusion 64x64\nprint(\"Processing COCO vs Stable Diffusion 64x64...\")\nprocess_images_in_batches(dataloader_coco_64, dataloader_stable_64, vgg16, svm_64, '/kaggle/working/metrics_coco_stable_6464.txt')\n\n# Comparison between COCO and Stable Diffusion 256x256\nprint(\"Processing COCO vs Stable Diffusion 256x256...\")\nprocess_images_in_batches(dataloader_coco_256, dataloader_stable_256, vgg16, svm_256, '/kaggle/working/metrics_coco_stable_256256.txt')\n\n\n\nprint(\"Processing complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T19:04:59.633156Z","iopub.status.idle":"2024-10-14T19:04:59.633646Z","shell.execute_reply.started":"2024-10-14T19:04:59.633377Z","shell.execute_reply":"2024-10-14T19:04:59.633401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Configuration to use GPU or CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nngpu = torch.cuda.device_count()\n\n# Load pre-trained VGG16 model\nvgg16 = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\nvgg16.classifier = vgg16.classifier[:-1]  # Remove the final classification layer\n\n# If multiple GPUs are available, use DataParallel\nif ngpu > 1:\n    vgg16 = torch.nn.DataParallel(vgg16)\n    print(f\"Using {ngpu} GPUs!\")\nelse:\n    print(\"Using a single GPU or CPU.\")\n\nvgg16.eval().to(device)  # Set the model to evaluation mode and move it to GPU/CPU\n\n# Function to preprocess images (with or without resize)\ndef preprocess_image(image, target_size=None):\n    \"\"\"\n    Preprocess an image by resizing (if size is provided) and normalizing it.\n    \n    Args:\n    - image (PIL.Image): Image to preprocess.\n    - target_size (tuple or None): Target size to pad the images to (width, height), or None to keep original size.\n    \n    Returns:\n    - Tensor: Preprocessed image as a Tensor.\n    \"\"\"\n    if target_size:\n        image = image.resize(target_size)  # Resize only if target_size is specified\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    return transform(image)\n\n# Custom dataset class to load images from a folder\nclass ImageDataset(Dataset):\n    \"\"\"\n    Custom dataset class to load images from a directory and preprocess them.\n    \n    Args:\n    - image_dir (str): Path to the directory containing images.\n    - target_size (tuple or None): The size to resize the images to (width, height), or None to keep original size.\n    \"\"\"\n    def __init__(self, image_dir, target_size=None):\n        self.image_dir = image_dir\n        self.target_size = target_size\n        self.image_names = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.image_names[idx])\n        image = Image.open(img_path).convert('RGB')\n        image = preprocess_image(image, self.target_size)\n        return image, self.image_names[idx]\n\n# Function to extract features using VGG16\ndef extract_features(images, model):\n    \"\"\"\n    Extract features from images using the VGG16 model.\n    \n    Args:\n    - images (Tensor): Batch of preprocessed images.\n    - model (torch.nn.Module): Pre-trained model (VGG16) for feature extraction.\n    \n    Returns:\n    - np.ndarray: Extracted features from the images.\n    \"\"\"\n    images = images.to(device)\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        features = model(images)\n    return features.flatten(1).cpu().numpy()\n\n# Function to calculate and save metrics\ndef calculate_metrics(y_true, y_pred, save_path):\n    \"\"\"\n    Calculate accuracy, precision, recall, and F1 score, then save the results to a file.\n    \n    Args:\n    - y_true (list): List of true (real) values.\n    - y_pred (list): List of predictions (generated by the model).\n    - save_path (str): Path to the file where the results will be saved.\n    \"\"\"\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n\n    # Save the results to a text file\n    with open(save_path, 'w') as f:\n        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n        f.write(f\"Precision: {precision:.4f}\\n\")\n        f.write(f\"Recall: {recall:.4f}\\n\")\n        f.write(f\"F1 Score: {f1:.4f}\\n\")\n\n    print(f\"Metrics saved to {save_path}\")\n\n# Function to process images in batches\ndef process_images_in_batches(dataloader_real, dataloader_generated, model, svm, result_file):\n    \"\"\"\n    Process real and generated images in batches, extract features, and compare predictions\n    using an SVM classifier.\n\n    Args:\n    - dataloader_real (DataLoader): Dataloader for real images.\n    - dataloader_generated (DataLoader): Dataloader for generated images.\n    - model (torch.nn.Module): Pre-trained model for feature extraction.\n    - svm (sklearn.svm.SVC): SVM classifier to compare the features.\n    - result_file (str): Path to the file where the metric results will be saved.\n    \"\"\"\n    y_true = []\n    y_pred = []\n\n    for real_batch, _ in dataloader_real:\n        real_batch = real_batch.to(device)\n        real_features = extract_features(real_batch, model)\n\n        for gen_batch, _ in dataloader_generated:\n            gen_batch = gen_batch.to(device)\n            gen_features = extract_features(gen_batch, model)\n\n            # Combine real and generated features\n            combined_features = torch.cat([torch.tensor(real_features), torch.tensor(gen_features)], dim=0).numpy()\n\n            # Predictions using the SVM classifier\n            predictions = svm.predict(combined_features)\n\n            # Add true values (1 for real, 0 for generated) and predictions\n            y_true.extend([1] * len(real_batch) + [0] * len(gen_batch))\n            y_pred.extend(predictions)\n\n            # Free GPU memory after each batch\n            torch.cuda.empty_cache()\n\n    calculate_metrics(y_true, y_pred, result_file)\n\n# Paths for the SVM checkpoint for mixed resolutions\nsvm_checkpoint_mixed = '/kaggle/input/checkpoint-classificatore/svm_classifier-2.pkl'  # Mixed classifier for different resolutions\n\n# Paths for the images (COCO, GAN 64x64, Stable Diffusion 256x256)\ncoco_dir = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\ngan_64x64_dir = '/kaggle/input/immagini-gan/kaggle/working/Immagini_gan/64x64'\nstable_256x256_dir = '/kaggle/input/immagini-stable/kaggle/working/Immagini_stable/256x256'\n\n# Reduced batch size to limit GPU memory usage\nbatch_size = 128  # Reduced to prevent memory errors\n\n# Dataset and DataLoader for COCO images (with padding to ensure uniform size, limit to 1000 images)\ndataset_coco_original = ImageDataset(coco_dir, target_size=(640, 640))\ndataset_coco_original = Subset(dataset_coco_original, range(1000))  # Limit to 1000 images\ndataloader_coco_original = DataLoader(dataset_coco_original, batch_size=batch_size, shuffle=False)\n\n# Dataset and DataLoader for GAN 64x64 images (limited to 1000 images)\ndataset_gan_64 = ImageDataset(gan_64x64_dir, target_size=(64, 64))\ndataset_gan_64 = Subset(dataset_gan_64, range(1000))\ndataloader_gan_64 = DataLoader(dataset_gan_64, batch_size=batch_size, shuffle=False)\n\n# Dataset and DataLoader for Stable Diffusion 256x256 images (limited to 1000 images)\ndataset_stable_256 = ImageDataset(stable_256x256_dir, target_size=(256, 256))\ndataset_stable_256 = Subset(dataset_stable_256, range(1000))\ndataloader_stable_256 = DataLoader(dataset_stable_256, batch_size=batch_size, shuffle=False)\n\n# Load the SVM classifier for mixed resolutions\nsvm_mixed = joblib.load(svm_checkpoint_mixed)\n\n# Comparison between COCO (no resize) and GAN 64x64\nprint(\"Processing COCO (no resize) vs GAN 64x64...\")\nprocess_images_in_batches(dataloader_coco_original, dataloader_gan_64, vgg16, svm_mixed, '/kaggle/working/metrics_coco_noresize_gan64.txt')\n\n# Comparison between COCO (no resize) and Stable Diffusion 256x256\nprint(\"Processing COCO (no resize) vs Stable Diffusion 256x256...\")\nprocess_images_in_batches(dataloader_coco_original, dataloader_stable_256, vgg16, svm_mixed, '/kaggle/working/metrics_coco_noresize_stable256.txt')\n\nprint(\"Processing complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T19:11:24.378721Z","iopub.execute_input":"2024-10-14T19:11:24.379633Z","iopub.status.idle":"2024-10-14T19:15:38.095923Z","shell.execute_reply.started":"2024-10-14T19:11:24.379590Z","shell.execute_reply":"2024-10-14T19:15:38.094996Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 209MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Using 2 GPUs!\nProcessing COCO (no resize) vs GAN 64x64...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"name":"stdout","text":"Metrics saved to /kaggle/working/metrics_coco_noresize_gan64.txt\nProcessing COCO (no resize) vs Stable Diffusion 256x256...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"name":"stdout","text":"Metrics saved to /kaggle/working/metrics_coco_noresize_stable256.txt\nProcessing complete.\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\n# Configurazione per l'uso di GPU o CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nngpu = torch.cuda.device_count()\n\n# Carica il modello VGG16 pre-addestrato\nvgg16 = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\nvgg16.classifier = vgg16.classifier[:-1]  # Rimuovi l'ultimo strato di classificazione\n\n# Se ci sono più GPU, utilizza DataParallel per parallelizzare il carico di lavoro\nif ngpu > 1:\n    vgg16 = torch.nn.DataParallel(vgg16)\nvgg16.eval().to(device)  # Imposta il modello in modalità di valutazione e spostalo su GPU/CPU\n\n# Funzione per preprocessare e comprimere le immagini (64x64)\ndef preprocess_and_compress_image(image, size=64, compression_quality=100):  # Compressione, 100 = nessuna compressione\n    \"\"\"\n    Preprocessa e comprime un'immagine ridimensionandola a una dimensione specificata, poi la salva con un livello\n    di compressione specifico e la ricarica per ulteriori operazioni.\n\n    Args:\n    - image (PIL.Image): Immagine da preprocessare.\n    - size (int): Dimensione a cui ridimensionare l'immagine (default 64).\n    - compression_quality (int): Qualità della compressione JPEG (100 = nessuna compressione).\n\n    Returns:\n    - Tensor: Immagine preprocessata e compressa.\n    \"\"\"\n    transform = transforms.Compose([\n        transforms.Resize((size, size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    # Comprime l'immagine utilizzando il formato JPEG con la qualità specificata\n    temp_img_path = '/kaggle/working/temp_image.jpg'\n    image = image.resize((size, size))  # Ridimensiona l'immagine\n    image.save(temp_img_path, format='JPEG', quality=compression_quality)  # Salva con la qualità specificata\n\n    # Ricarica l'immagine compressa e preprocessala\n    compressed_image = Image.open(temp_img_path).convert('RGB')\n    return transform(compressed_image)\n\n# Dataset personalizzato per caricare le immagini da una cartella e applicare la compressione\nclass ImageDataset(Dataset):\n    \"\"\"\n    Classe per creare un dataset personalizzato che carica immagini da una directory e applica\n    la compressione e il preprocessing.\n\n    Args:\n    - image_dir (str): Percorso alla directory delle immagini.\n    - size (int): Dimensione a cui ridimensionare le immagini.\n    - compression_quality (int): Qualità della compressione JPEG.\n    \"\"\"\n    def __init__(self, image_dir, size, compression_quality=100):\n        self.image_dir = image_dir\n        self.size = size\n        self.compression_quality = compression_quality\n        self.image_names = [f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.image_names[idx])\n        image = Image.open(img_path).convert('RGB')\n        image = preprocess_and_compress_image(image, self.size, self.compression_quality)\n        return image, self.image_names[idx]\n\n# Funzione per estrarre le caratteristiche con VGG16\ndef extract_features(images, model):\n    \"\"\"\n    Estrae le caratteristiche dalle immagini utilizzando il modello VGG16.\n\n    Args:\n    - images (Tensor): Batch di immagini preprocessate.\n    - model (torch.nn.Module): Modello pre-addestrato per l'estrazione delle caratteristiche.\n\n    Returns:\n    - np.ndarray: Caratteristiche estratte dalle immagini.\n    \"\"\"\n    images = images.to(device)\n    with torch.no_grad():  # Disabilita il calcolo dei gradienti per risparmiare memoria\n        features = model(images)\n    return features.flatten(1).cpu().numpy()\n\n# Funzione per calcolare e salvare le metriche\ndef calculate_metrics(y_true, y_pred, save_path):\n    \"\"\"\n    Calcola l'accuratezza, precisione, richiamo e F1 score e salva i risultati in un file di testo.\n\n    Args:\n    - y_true (list): Lista dei valori reali (1 per reale, 0 per generato).\n    - y_pred (list): Lista delle predizioni effettuate dal modello.\n    - save_path (str): Percorso del file dove salvare i risultati.\n    \"\"\"\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n\n    # Salva i risultati in un file di testo\n    with open(save_path, 'w') as f:\n        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n        f.write(f\"Precision: {precision:.4f}\\n\")\n        f.write(f\"Recall: {recall:.4f}\\n\")\n        f.write(f\"F1 Score: {f1:.4f}\\n\")\n\n    print(f\"Metrics saved to {save_path}\")\n\n# Funzione per processare le immagini in batch\ndef process_images_in_batches(dataloader_real, dataloader_generated, model, svm, result_file):\n    \"\"\"\n    Processa le immagini reali e generate in batch, estrae le caratteristiche e confronta le predizioni\n    utilizzando un classificatore SVM.\n\n    Args:\n    - dataloader_real (DataLoader): Dataloader per le immagini reali.\n    - dataloader_generated (DataLoader): Dataloader per le immagini generate.\n    - model (torch.nn.Module): Modello pre-addestrato per l'estrazione delle caratteristiche.\n    - svm (sklearn.svm.SVC): Classificatore SVM per confrontare le caratteristiche.\n    - result_file (str): Percorso del file dove salvare i risultati delle metriche.\n    \"\"\"\n    y_true = []\n    y_pred = []\n\n    for real_batch, _ in dataloader_real:\n        real_batch = real_batch.to(device)\n        real_features = extract_features(real_batch, model)\n\n        for gen_batch, _ in dataloader_generated:\n            gen_batch = gen_batch.to(device)\n            gen_features = extract_features(gen_batch, model)\n\n            combined_features = torch.cat([torch.tensor(real_features), torch.tensor(gen_features)], dim=0).numpy()\n\n            predictions = svm.predict(combined_features)\n\n            y_true.extend([1] * len(real_batch) + [0] * len(gen_batch))\n            y_pred.extend(predictions)\n\n            # Libera la memoria GPU dopo ogni batch\n            torch.cuda.empty_cache()\n\n    calculate_metrics(y_true, y_pred, result_file)\n\n# Percorsi per i checkpoint SVM\nsvm_checkpoint_64 = '/kaggle/input/checkpoint-classificatore-6464/svm_classifier_64.pkl'\n\n# Percorsi per le immagini ridimensionate\ncoco_dir = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\nstable_64x64_dir = '/kaggle/input/immagini-stable/kaggle/working/Immagini_stable/64x64'\ngan_64x64_dir = '/kaggle/input/immagini-gan/kaggle/working/Immagini_gan/64x64'\n\n# Batch size\nbatch_size = 128\n\n# Livelli di compressione\ncompression_levels = {\n    '40': 60,  # 40% di compressione (60% qualità)\n    '60': 40,  # 60% di compressione (40% qualità)\n    '80': 20   # 80% di compressione (20% qualità)\n}\n\n# Dataset e DataLoader per le immagini COCO 64x64 (limite a 1000 immagini)\ndataset_coco_64 = ImageDataset(coco_dir, 64, compression_quality=100)  # Nessuna compressione per COCO\ndataset_coco_64 = Subset(dataset_coco_64, range(1000))\ndataloader_coco_64 = DataLoader(dataset_coco_64, batch_size=batch_size, shuffle=False)\n\n# Carica il classificatore SVM per 64x64\nsvm_64 = joblib.load(svm_checkpoint_64)\n\n# Itera sui diversi livelli di compressione\nfor level, quality in compression_levels.items():\n    # Dataset e DataLoader per il dataset misto GAN e Stable (1000 immagini da ciascuno)\n    dataset_stable_64 = ImageDataset(stable_64x64_dir, 64, compression_quality=quality)\n    dataset_stable_64 = Subset(dataset_stable_64, range(1000))\n\n    dataset_gan_64 = ImageDataset(gan_64x64_dir, 64, compression_quality=quality)\n    dataset_gan_64 = Subset(dataset_gan_64, range(1000))\n\n    # Unisci i dataset GAN e Stable in un unico DataLoader\n    combined_dataset = dataset_stable_64 + dataset_gan_64\n    dataloader_combined = DataLoader(combined_dataset, batch_size=batch_size, shuffle=False)\n\n    # Confronto tra COCO e il dataset misto (64x64 con compressione)\n    print(f\"Processing COCO vs GAN + Stable Diffusion (64x64, compressione {level}%)...\")\n    result_file = f'/kaggle/working/metrics_coco_combined_6464_compression_{level}.txt'\n    process_images_in_batches(dataloader_coco_64, dataloader_combined, vgg16, svm_64, result_file)\n\nprint(\"Processing complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T19:05:01.603618Z","iopub.execute_input":"2024-10-14T19:05:01.604022Z","iopub.status.idle":"2024-10-14T19:05:36.934385Z","shell.execute_reply.started":"2024-10-14T19:05:01.603978Z","shell.execute_reply":"2024-10-14T19:05:36.933092Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Processing COCO vs GAN + Stable Diffusion (64x64, compressione 40%)...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 188\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing COCO vs GAN + Stable Diffusion (64x64, compressione \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    187\u001b[0m     result_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/working/metrics_coco_combined_6464_compression_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 188\u001b[0m     \u001b[43mprocess_images_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader_coco_64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_combined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvgg16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msvm_64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[5], line 130\u001b[0m, in \u001b[0;36mprocess_images_in_batches\u001b[0;34m(dataloader_real, dataloader_generated, model, svm, result_file)\u001b[0m\n\u001b[1;32m    127\u001b[0m real_batch \u001b[38;5;241m=\u001b[39m real_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    128\u001b[0m real_features \u001b[38;5;241m=\u001b[39m extract_features(real_batch, model)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gen_batch, _ \u001b[38;5;129;01min\u001b[39;00m dataloader_generated:\n\u001b[1;32m    131\u001b[0m     gen_batch \u001b[38;5;241m=\u001b[39m gen_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    132\u001b[0m     gen_features \u001b[38;5;241m=\u001b[39m extract_features(gen_batch, model)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:350\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     sample_idx \u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[dataset_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43msample_idx\u001b[49m\u001b[43m]\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:412\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n","Cell \u001b[0;32mIn[5], line 66\u001b[0m, in \u001b[0;36mImageDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     64\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_names[idx])\n\u001b[1;32m     65\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(img_path)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_and_compress_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression_quality\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_names[idx]\n","Cell \u001b[0;32mIn[5], line 40\u001b[0m, in \u001b[0;36mpreprocess_and_compress_image\u001b[0;34m(image, size, compression_quality)\u001b[0m\n\u001b[1;32m     37\u001b[0m image\u001b[38;5;241m.\u001b[39msave(temp_img_path, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJPEG\u001b[39m\u001b[38;5;124m'\u001b[39m, quality\u001b[38;5;241m=\u001b[39mcompression_quality)  \u001b[38;5;66;03m# Salva con la qualità specificata\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Ricarica l'immagine compressa e preprocessala\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m compressed_image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_img_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transform(compressed_image)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3477\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3474\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   3475\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3477\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43m_open_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m formats \u001b[38;5;129;01mis\u001b[39;00m ID:\n\u001b[1;32m   3480\u001b[0m     checked_formats \u001b[38;5;241m=\u001b[39m ID\u001b[38;5;241m.\u001b[39mcopy()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3465\u001b[0m, in \u001b[0;36mopen.<locals>._open_core\u001b[0;34m(fp, filename, prefix, formats)\u001b[0m\n\u001b[1;32m   3463\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m result:\n\u001b[1;32m   3464\u001b[0m     fp\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 3465\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3466\u001b[0m     _decompression_bomb_check(im\u001b[38;5;241m.\u001b[39msize)\n\u001b[1;32m   3467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/JpegImagePlugin.py:829\u001b[0m, in \u001b[0;36mjpeg_factory\u001b[0;34m(fp, filename)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjpeg_factory\u001b[39m(fp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 829\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[43mJpegImageFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m         mpheader \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39m_getmp()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/ImageFile.py:138\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;167;01mIndexError\u001b[39;00m,  \u001b[38;5;66;03m# end of data\u001b[39;00m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;167;01mTypeError\u001b[39;00m,  \u001b[38;5;66;03m# end of data (ord)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m         struct\u001b[38;5;241m.\u001b[39merror,\n\u001b[1;32m    145\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m v:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m(v) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mv\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/JpegImagePlugin.py:355\u001b[0m, in \u001b[0;36mJpegImageFile._open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0xFF\u001b[39m:\n\u001b[1;32m    354\u001b[0m     s \u001b[38;5;241m=\u001b[39m s \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 355\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[43mi16\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# Skip non-0xFF junk\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m1\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/_binary.py:90\u001b[0m, in \u001b[0;36mi16be\u001b[0;34m(c, o)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m    Converts a 4-bytes (32 bits) string to a signed integer, big endian.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    :param c: string containing bytes to convert\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m    :param o: offset of bytes to convert in string\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unpack_from(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>i\u001b[39m\u001b[38;5;124m\"\u001b[39m, c, o)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mi16be\u001b[39m(c: \u001b[38;5;28mbytes\u001b[39m, o: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unpack_from(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>H\u001b[39m\u001b[38;5;124m\"\u001b[39m, c, o)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mi32be\u001b[39m(c: \u001b[38;5;28mbytes\u001b[39m, o: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}